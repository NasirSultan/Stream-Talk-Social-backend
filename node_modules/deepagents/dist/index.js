import { AIMessage, HumanMessage, SystemMessage, ToolMessage, anthropicPromptCachingMiddleware, createAgent, createMiddleware, humanInTheLoopMiddleware, summarizationMiddleware, todoListMiddleware, tool } from "langchain";
import { Runnable } from "@langchain/core/runnables";
import { Command, REMOVE_ALL_MESSAGES, ReducedValue, StateSchema, getCurrentTaskInput, isCommand } from "@langchain/langgraph";
import { z } from "zod/v4";
import micromatch from "micromatch";
import { basename } from "path";
import { HumanMessage as HumanMessage$1, RemoveMessage } from "@langchain/core/messages";
import { z as z$1 } from "zod";
import yaml from "yaml";
import "uuid";
import "langchain/chat_models/universal";
import fs from "node:fs/promises";
import fs$1 from "node:fs";
import path from "node:path";
import { spawn } from "node:child_process";
import fg from "fast-glob";
import os from "node:os";

//#region src/backends/protocol.ts
/**
* Type guard to check if a backend supports execution.
*
* @param backend - Backend instance to check
* @returns True if the backend implements SandboxBackendProtocol
*/
function isSandboxBackend(backend) {
	return typeof backend.execute === "function" && typeof backend.id === "string";
}
const SANDBOX_ERROR_SYMBOL = Symbol.for("sandbox.error");
/**
* Custom error class for sandbox operations.
*
* @param message - Human-readable error description
* @param code - Structured error code for programmatic handling
* @returns SandboxError with message and code
*
* @example
* ```typescript
* try {
*   await sandbox.execute("some command");
* } catch (error) {
*   if (error instanceof SandboxError) {
*     switch (error.code) {
*       case "NOT_INITIALIZED":
*         await sandbox.initialize();
*         break;
*       case "COMMAND_TIMEOUT":
*         console.error("Command took too long");
*         break;
*       default:
*         throw error;
*     }
*   }
* }
* ```
*/
var SandboxError = class SandboxError extends Error {
	/** Symbol for identifying sandbox error instances */
	[SANDBOX_ERROR_SYMBOL] = true;
	/** Error name for instanceof checks and logging */
	name = "SandboxError";
	/**
	* Creates a new SandboxError.
	*
	* @param message - Human-readable error description
	* @param code - Structured error code for programmatic handling
	*/
	constructor(message, code, cause) {
		super(message);
		this.code = code;
		this.cause = cause;
		Object.setPrototypeOf(this, SandboxError.prototype);
	}
	static isInstance(error) {
		return typeof error === "object" && error !== null && error[SANDBOX_ERROR_SYMBOL] === true;
	}
};

//#endregion
//#region src/backends/utils.ts
/**
* Shared utility functions for memory backend implementations.
*
* This module contains both user-facing string formatters and structured
* helpers used by backends and the composite router. Structured helpers
* enable composition without fragile string parsing.
*/
const EMPTY_CONTENT_WARNING = "System reminder: File exists but has empty contents";
const MAX_LINE_LENGTH = 1e4;
const LINE_NUMBER_WIDTH = 6;
/**
* Sanitize tool_call_id to prevent path traversal and separator issues.
*
* Replaces dangerous characters (., /, \) with underscores.
*/
function sanitizeToolCallId(toolCallId) {
	return toolCallId.replace(/\./g, "_").replace(/\//g, "_").replace(/\\/g, "_");
}
/**
* Format file content with line numbers (cat -n style).
*
* Chunks lines longer than MAX_LINE_LENGTH with continuation markers (e.g., 5.1, 5.2).
*
* @param content - File content as string or list of lines
* @param startLine - Starting line number (default: 1)
* @returns Formatted content with line numbers and continuation markers
*/
function formatContentWithLineNumbers(content, startLine = 1) {
	let lines;
	if (typeof content === "string") {
		lines = content.split("\n");
		if (lines.length > 0 && lines[lines.length - 1] === "") lines = lines.slice(0, -1);
	} else lines = content;
	const resultLines = [];
	for (let i = 0; i < lines.length; i++) {
		const line = lines[i];
		const lineNum = i + startLine;
		if (line.length <= MAX_LINE_LENGTH) resultLines.push(`${lineNum.toString().padStart(LINE_NUMBER_WIDTH)}\t${line}`);
		else {
			const numChunks = Math.ceil(line.length / MAX_LINE_LENGTH);
			for (let chunkIdx = 0; chunkIdx < numChunks; chunkIdx++) {
				const start = chunkIdx * MAX_LINE_LENGTH;
				const end = Math.min(start + MAX_LINE_LENGTH, line.length);
				const chunk = line.substring(start, end);
				if (chunkIdx === 0) resultLines.push(`${lineNum.toString().padStart(LINE_NUMBER_WIDTH)}\t${chunk}`);
				else {
					const continuationMarker = `${lineNum}.${chunkIdx}`;
					resultLines.push(`${continuationMarker.padStart(LINE_NUMBER_WIDTH)}\t${chunk}`);
				}
			}
		}
	}
	return resultLines.join("\n");
}
/**
* Check if content is empty and return warning message.
*
* @param content - Content to check
* @returns Warning message if empty, null otherwise
*/
function checkEmptyContent(content) {
	if (!content || content.trim() === "") return EMPTY_CONTENT_WARNING;
	return null;
}
/**
* Convert FileData to plain string content.
*
* @param fileData - FileData object with 'content' key
* @returns Content as string with lines joined by newlines
*/
function fileDataToString(fileData) {
	return fileData.content.join("\n");
}
/**
* Create a FileData object with timestamps.
*
* @param content - File content as string
* @param createdAt - Optional creation timestamp (ISO format)
* @returns FileData object with content and timestamps
*/
function createFileData(content, createdAt) {
	const lines = typeof content === "string" ? content.split("\n") : content;
	const now = (/* @__PURE__ */ new Date()).toISOString();
	return {
		content: lines,
		created_at: createdAt || now,
		modified_at: now
	};
}
/**
* Update FileData with new content, preserving creation timestamp.
*
* @param fileData - Existing FileData object
* @param content - New content as string
* @returns Updated FileData object
*/
function updateFileData(fileData, content) {
	const lines = typeof content === "string" ? content.split("\n") : content;
	const now = (/* @__PURE__ */ new Date()).toISOString();
	return {
		content: lines,
		created_at: fileData.created_at,
		modified_at: now
	};
}
/**
* Format file data for read response with line numbers.
*
* @param fileData - FileData object
* @param offset - Line offset (0-indexed)
* @param limit - Maximum number of lines
* @returns Formatted content or error message
*/
function formatReadResponse(fileData, offset, limit) {
	const content = fileDataToString(fileData);
	const emptyMsg = checkEmptyContent(content);
	if (emptyMsg) return emptyMsg;
	const lines = content.split("\n");
	const startIdx = offset;
	const endIdx = Math.min(startIdx + limit, lines.length);
	if (startIdx >= lines.length) return `Error: Line offset ${offset} exceeds file length (${lines.length} lines)`;
	return formatContentWithLineNumbers(lines.slice(startIdx, endIdx), startIdx + 1);
}
/**
* Perform string replacement with occurrence validation.
*
* @param content - Original content
* @param oldString - String to replace
* @param newString - Replacement string
* @param replaceAll - Whether to replace all occurrences
* @returns Tuple of [new_content, occurrences] on success, or error message string
*
* Special case: When both content and oldString are empty, this sets the initial
* content to newString. This allows editing empty files by treating empty oldString
* as "set initial content" rather than "replace nothing".
*/
function performStringReplacement(content, oldString, newString, replaceAll) {
	if (content === "" && oldString === "") return [newString, 0];
	if (oldString === "") return "Error: oldString cannot be empty when file has content";
	const occurrences = content.split(oldString).length - 1;
	if (occurrences === 0) return `Error: String not found in file: '${oldString}'`;
	if (occurrences > 1 && !replaceAll) return `Error: String '${oldString}' appears ${occurrences} times in file. Use replace_all=True to replace all instances, or provide a more specific string with surrounding context.`;
	return [content.split(oldString).join(newString), occurrences];
}
/**
* Validate and normalize a directory path.
*
* Ensures paths are safe to use by preventing directory traversal attacks
* and enforcing consistent formatting. All paths are normalized to use
* forward slashes and start with a leading slash.
*
* This function is designed for virtual filesystem paths and rejects
* Windows absolute paths (e.g., C:/..., F:/...) to maintain consistency
* and prevent path format ambiguity.
*
* @param path - Path to validate
* @returns Normalized path starting with / and ending with /
* @throws Error if path is invalid
*
* @example
* ```typescript
* validatePath("foo/bar")  // Returns: "/foo/bar/"
* validatePath("/./foo//bar")  // Returns: "/foo/bar/"
* validatePath("../etc/passwd")  // Throws: Path traversal not allowed
* validatePath("C:\\Users\\file")  // Throws: Windows absolute paths not supported
* ```
*/
function validatePath(path) {
	const pathStr = path || "/";
	if (!pathStr || pathStr.trim() === "") throw new Error("Path cannot be empty");
	let normalized = pathStr.startsWith("/") ? pathStr : "/" + pathStr;
	if (!normalized.endsWith("/")) normalized += "/";
	return normalized;
}
/**
* Search files dict for paths matching glob pattern.
*
* @param files - Dictionary of file paths to FileData
* @param pattern - Glob pattern (e.g., `*.py`, `**\/*.ts`)
* @param path - Base path to search from
* @returns Newline-separated file paths, sorted by modification time (most recent first).
*          Returns "No files found" if no matches.
*
* @example
* ```typescript
* const files = {"/src/main.py": FileData(...), "/test.py": FileData(...)};
* globSearchFiles(files, "*.py", "/");
* // Returns: "/test.py\n/src/main.py" (sorted by modified_at)
* ```
*/
function globSearchFiles(files, pattern, path = "/") {
	let normalizedPath;
	try {
		normalizedPath = validatePath(path);
	} catch {
		return "No files found";
	}
	const filtered = Object.fromEntries(Object.entries(files).filter(([fp]) => fp.startsWith(normalizedPath)));
	const effectivePattern = pattern;
	const matches = [];
	for (const [filePath, fileData] of Object.entries(filtered)) {
		let relative = filePath.substring(normalizedPath.length);
		if (relative.startsWith("/")) relative = relative.substring(1);
		if (!relative) {
			const parts = filePath.split("/");
			relative = parts[parts.length - 1] || "";
		}
		if (micromatch.isMatch(relative, effectivePattern, {
			dot: true,
			nobrace: false
		})) matches.push([filePath, fileData.modified_at]);
	}
	matches.sort((a, b) => b[1].localeCompare(a[1]));
	if (matches.length === 0) return "No files found";
	return matches.map(([fp]) => fp).join("\n");
}
/**
* Return structured grep matches from an in-memory files mapping.
*
* Performs literal text search (not regex).
*
* Returns a list of GrepMatch on success, or a string for invalid inputs.
* We deliberately do not raise here to keep backends non-throwing in tool
* contexts and preserve user-facing error messages.
*/
function grepMatchesFromFiles(files, pattern, path = null, glob = null) {
	let normalizedPath;
	try {
		normalizedPath = validatePath(path);
	} catch {
		return [];
	}
	let filtered = Object.fromEntries(Object.entries(files).filter(([fp]) => fp.startsWith(normalizedPath)));
	if (glob) filtered = Object.fromEntries(Object.entries(filtered).filter(([fp]) => micromatch.isMatch(basename(fp), glob, {
		dot: true,
		nobrace: false
	})));
	const matches = [];
	for (const [filePath, fileData] of Object.entries(filtered)) for (let i = 0; i < fileData.content.length; i++) {
		const line = fileData.content[i];
		const lineNum = i + 1;
		if (line.includes(pattern)) matches.push({
			path: filePath,
			line: lineNum,
			text: line
		});
	}
	return matches;
}

//#endregion
//#region src/backends/state.ts
/**
* Backend that stores files in agent state (ephemeral).
*
* Uses LangGraph's state management and checkpointing. Files persist within
* a conversation thread but not across threads. State is automatically
* checkpointed after each agent step.
*
* Special handling: Since LangGraph state must be updated via Command objects
* (not direct mutation), operations return filesUpdate in WriteResult/EditResult
* for the middleware to apply via Command.
*/
var StateBackend = class {
	stateAndStore;
	constructor(stateAndStore) {
		this.stateAndStore = stateAndStore;
	}
	/**
	* Get files from current state.
	*/
	getFiles() {
		return this.stateAndStore.state.files || {};
	}
	/**
	* List files and directories in the specified directory (non-recursive).
	*
	* @param path - Absolute path to directory
	* @returns List of FileInfo objects for files and directories directly in the directory.
	*          Directories have a trailing / in their path and is_dir=true.
	*/
	lsInfo(path) {
		const files = this.getFiles();
		const infos = [];
		const subdirs = /* @__PURE__ */ new Set();
		const normalizedPath = path.endsWith("/") ? path : path + "/";
		for (const [k, fd] of Object.entries(files)) {
			if (!k.startsWith(normalizedPath)) continue;
			const relative = k.substring(normalizedPath.length);
			if (relative.includes("/")) {
				const subdirName = relative.split("/")[0];
				subdirs.add(normalizedPath + subdirName + "/");
				continue;
			}
			const size = fd.content.join("\n").length;
			infos.push({
				path: k,
				is_dir: false,
				size,
				modified_at: fd.modified_at
			});
		}
		for (const subdir of Array.from(subdirs).sort()) infos.push({
			path: subdir,
			is_dir: true,
			size: 0,
			modified_at: ""
		});
		infos.sort((a, b) => a.path.localeCompare(b.path));
		return infos;
	}
	/**
	* Read file content with line numbers.
	*
	* @param filePath - Absolute file path
	* @param offset - Line offset to start reading from (0-indexed)
	* @param limit - Maximum number of lines to read
	* @returns Formatted file content with line numbers, or error message
	*/
	read(filePath, offset = 0, limit = 500) {
		const fileData = this.getFiles()[filePath];
		if (!fileData) return `Error: File '${filePath}' not found`;
		return formatReadResponse(fileData, offset, limit);
	}
	/**
	* Read file content as raw FileData.
	*
	* @param filePath - Absolute file path
	* @returns Raw file content as FileData
	*/
	readRaw(filePath) {
		const fileData = this.getFiles()[filePath];
		if (!fileData) throw new Error(`File '${filePath}' not found`);
		return fileData;
	}
	/**
	* Create a new file with content.
	* Returns WriteResult with filesUpdate to update LangGraph state.
	*/
	write(filePath, content) {
		if (filePath in this.getFiles()) return { error: `Cannot write to ${filePath} because it already exists. Read and then make an edit, or write to a new path.` };
		const newFileData = createFileData(content);
		return {
			path: filePath,
			filesUpdate: { [filePath]: newFileData }
		};
	}
	/**
	* Edit a file by replacing string occurrences.
	* Returns EditResult with filesUpdate and occurrences.
	*/
	edit(filePath, oldString, newString, replaceAll = false) {
		const fileData = this.getFiles()[filePath];
		if (!fileData) return { error: `Error: File '${filePath}' not found` };
		const result = performStringReplacement(fileDataToString(fileData), oldString, newString, replaceAll);
		if (typeof result === "string") return { error: result };
		const [newContent, occurrences] = result;
		const newFileData = updateFileData(fileData, newContent);
		return {
			path: filePath,
			filesUpdate: { [filePath]: newFileData },
			occurrences
		};
	}
	/**
	* Structured search results or error string for invalid input.
	*/
	grepRaw(pattern, path = "/", glob = null) {
		return grepMatchesFromFiles(this.getFiles(), pattern, path, glob);
	}
	/**
	* Structured glob matching returning FileInfo objects.
	*/
	globInfo(pattern, path = "/") {
		const files = this.getFiles();
		const result = globSearchFiles(files, pattern, path);
		if (result === "No files found") return [];
		const paths = result.split("\n");
		const infos = [];
		for (const p of paths) {
			const fd = files[p];
			const size = fd ? fd.content.join("\n").length : 0;
			infos.push({
				path: p,
				is_dir: false,
				size,
				modified_at: fd?.modified_at || ""
			});
		}
		return infos;
	}
	/**
	* Upload multiple files.
	*
	* Note: Since LangGraph state must be updated via Command objects,
	* the caller must apply filesUpdate via Command after calling this method.
	*
	* @param files - List of [path, content] tuples to upload
	* @returns List of FileUploadResponse objects, one per input file
	*/
	uploadFiles(files) {
		const responses = [];
		const updates = {};
		for (const [path, content] of files) try {
			updates[path] = createFileData(new TextDecoder().decode(content));
			responses.push({
				path,
				error: null
			});
		} catch {
			responses.push({
				path,
				error: "invalid_path"
			});
		}
		const result = responses;
		result.filesUpdate = updates;
		return result;
	}
	/**
	* Download multiple files.
	*
	* @param paths - List of file paths to download
	* @returns List of FileDownloadResponse objects, one per input path
	*/
	downloadFiles(paths) {
		const files = this.getFiles();
		const responses = [];
		for (const path of paths) {
			const fileData = files[path];
			if (!fileData) {
				responses.push({
					path,
					content: null,
					error: "file_not_found"
				});
				continue;
			}
			const contentStr = fileDataToString(fileData);
			const content = new TextEncoder().encode(contentStr);
			responses.push({
				path,
				content,
				error: null
			});
		}
		return responses;
	}
};

//#endregion
//#region src/middleware/fs.ts
/**
* Middleware for providing filesystem tools to an agent.
*
* Provides ls, read_file, write_file, edit_file, glob, and grep tools with support for:
* - Pluggable backends (StateBackend, StoreBackend, FilesystemBackend, CompositeBackend)
* - Tool result eviction for large outputs
*/
/**
* Tools that should be excluded from the large result eviction logic.
*
* This array contains tools that should NOT have their results evicted to the filesystem
* when they exceed token limits. Tools are excluded for different reasons:
*
* 1. Tools with built-in truncation (ls, glob, grep):
*    These tools truncate their own output when it becomes too large. When these tools
*    produce truncated output due to many matches, it typically indicates the query
*    needs refinement rather than full result preservation. In such cases, the truncated
*    matches are potentially more like noise and the LLM should be prompted to narrow
*    its search criteria instead.
*
* 2. Tools with problematic truncation behavior (read_file):
*    read_file is tricky to handle as the failure mode here is single long lines
*    (e.g., imagine a jsonl file with very long payloads on each line). If we try to
*    truncate the result of read_file, the agent may then attempt to re-read the
*    truncated file using read_file again, which won't help.
*
* 3. Tools that never exceed limits (edit_file, write_file):
*    These tools return minimal confirmation messages and are never expected to produce
*    output large enough to exceed token limits, so checking them would be unnecessary.
*/
const TOOLS_EXCLUDED_FROM_EVICTION = [
	"ls",
	"glob",
	"grep",
	"read_file",
	"edit_file",
	"write_file"
];
/**
* Approximate number of characters per token for truncation calculations.
* Using 4 chars per token as a conservative approximation (actual ratio varies by content)
* This errs on the high side to avoid premature eviction of content that might fit.
*/
const NUM_CHARS_PER_TOKEN = 4;
/**
* Default values for read_file tool pagination (in lines).
*/
const DEFAULT_READ_LINE_OFFSET = 0;
const DEFAULT_READ_LINE_LIMIT = 100;
/**
* Template for truncation message in read_file.
* {file_path} will be filled in at runtime.
*/
const READ_FILE_TRUNCATION_MSG = `

[Output was truncated due to size limits. The file content is very large. Consider reformatting the file to make it easier to navigate. For example, if this is JSON, use execute(command='jq . {file_path}') to pretty-print it with line breaks. For other formats, you can use appropriate formatting tools to split long lines.]`;
/**
* Message template for evicted tool results.
*/
const TOO_LARGE_TOOL_MSG = `Tool result too large, the result of this tool call {tool_call_id} was saved in the filesystem at this path: {file_path}
You can read the result from the filesystem by using the read_file tool, but make sure to only read part of the result at a time.
You can do this by specifying an offset and limit in the read_file tool call.
For example, to read the first 100 lines, you can use the read_file tool with offset=0 and limit=100.

Here is a preview showing the head and tail of the result (lines of the form
... [N lines truncated] ...
indicate omitted lines in the middle of the content):

{content_sample}`;
/**
* Create a preview of content showing head and tail with truncation marker.
*
* @param contentStr - The full content string to preview.
* @param headLines - Number of lines to show from the start (default: 5).
* @param tailLines - Number of lines to show from the end (default: 5).
* @returns Formatted preview string with line numbers.
*/
function createContentPreview(contentStr, headLines = 5, tailLines = 5) {
	const lines = contentStr.split("\n");
	if (lines.length <= headLines + tailLines) return formatContentWithLineNumbers(lines.map((line) => line.substring(0, 1e3)), 1);
	const head = lines.slice(0, headLines).map((line) => line.substring(0, 1e3));
	const tail = lines.slice(-tailLines).map((line) => line.substring(0, 1e3));
	const headSample = formatContentWithLineNumbers(head, 1);
	const truncationNotice = `\n... [${lines.length - headLines - tailLines} lines truncated] ...\n`;
	const tailSample = formatContentWithLineNumbers(tail, lines.length - tailLines + 1);
	return headSample + truncationNotice + tailSample;
}
/**
* Zod v3 schema for FileData (re-export from backends)
*/
const FileDataSchema = z.object({
	content: z.array(z.string()),
	created_at: z.string(),
	modified_at: z.string()
});
/**
* Reducer for files state that merges file updates with support for deletions.
* When a file value is null, the file is deleted from state.
* When a file value is non-null, it is added or updated in state.
*
* This reducer enables concurrent updates from parallel subagents by properly
* merging their file changes instead of requiring LastValue semantics.
*
* @param current - The current files record (from state)
* @param update - The new files record (from a subagent update), with null values for deletions
* @returns Merged files record with deletions applied
*/
function fileDataReducer(current, update) {
	if (update === void 0) return current || {};
	if (current === void 0) {
		const result = {};
		for (const [key, value] of Object.entries(update)) if (value !== null) result[key] = value;
		return result;
	}
	const result = { ...current };
	for (const [key, value] of Object.entries(update)) if (value === null) delete result[key];
	else result[key] = value;
	return result;
}
/**
* Shared filesystem state schema.
* Defined at module level to ensure the same object identity is used across all agents,
* preventing "Channel already exists with different type" errors when multiple agents
* use createFilesystemMiddleware.
*
* Uses ReducedValue for files to allow concurrent updates from parallel subagents.
*/
const FilesystemStateSchema = new StateSchema({ files: new ReducedValue(z.record(z.string(), FileDataSchema).default(() => ({})), {
	inputSchema: z.record(z.string(), FileDataSchema.nullable()).optional(),
	reducer: fileDataReducer
}) });
/**
* Resolve backend from factory or instance.
*
* @param backend - Backend instance or factory function
* @param stateAndStore - State and store container for backend initialization
*/
function getBackend(backend, stateAndStore) {
	if (typeof backend === "function") return backend(stateAndStore);
	return backend;
}
const FILESYSTEM_SYSTEM_PROMPT = `## Filesystem Tools \`ls\`, \`read_file\`, \`write_file\`, \`edit_file\`, \`glob\`, \`grep\`

You have access to a filesystem which you can interact with using these tools.
All file paths must start with a /.

- ls: list files in a directory (requires absolute path)
- read_file: read a file from the filesystem
- write_file: write to a file in the filesystem
- edit_file: edit a file in the filesystem
- glob: find files matching a pattern (e.g., "**/*.py")
- grep: search for text within files`;
const LS_TOOL_DESCRIPTION = `Lists all files in a directory.

This is useful for exploring the filesystem and finding the right file to read or edit.
You should almost ALWAYS use this tool before using the read_file or edit_file tools.`;
const READ_FILE_TOOL_DESCRIPTION = `Reads a file from the filesystem.

Assume this tool is able to read all files. If the User provides a path to a file assume that path is valid. It is okay to read a file that does not exist; an error will be returned.

Usage:
- By default, it reads up to 100 lines starting from the beginning of the file
- **IMPORTANT for large files and codebase exploration**: Use pagination with offset and limit parameters to avoid context overflow
  - First scan: read_file(path, limit=100) to see file structure
  - Read more sections: read_file(path, offset=100, limit=200) for next 200 lines
  - Only omit limit (read full file) when necessary for editing
- Specify offset and limit: read_file(path, offset=0, limit=100) reads first 100 lines
- Results are returned using cat -n format, with line numbers starting at 1
- Lines longer than 10,000 characters will be split into multiple lines with continuation markers (e.g., 5.1, 5.2, etc.). When you specify a limit, these continuation lines count towards the limit.
- You have the capability to call multiple tools in a single response. It is always better to speculatively read multiple files as a batch that are potentially useful.
- If you read a file that exists but has empty contents you will receive a system reminder warning in place of file contents.
- You should ALWAYS make sure a file has been read before editing it.`;
const WRITE_FILE_TOOL_DESCRIPTION = `Writes to a new file in the filesystem.

Usage:
- The write_file tool will create a new file.
- Prefer to edit existing files (with the edit_file tool) over creating new ones when possible.`;
const EDIT_FILE_TOOL_DESCRIPTION = `Performs exact string replacements in files.

Usage:
- You must read the file before editing. This tool will error if you attempt an edit without reading the file first.
- When editing, preserve the exact indentation (tabs/spaces) from the read output. Never include line number prefixes in old_string or new_string.
- ALWAYS prefer editing existing files over creating new ones.
- Only use emojis if the user explicitly requests it.`;
const GLOB_TOOL_DESCRIPTION = `Find files matching a glob pattern.

Supports standard glob patterns: \`*\` (any characters), \`**\` (any directories), \`?\` (single character).
Returns a list of absolute file paths that match the pattern.

Examples:
- \`**/*.py\` - Find all Python files
- \`*.txt\` - Find all text files in root
- \`/subdir/**/*.md\` - Find all markdown files under /subdir`;
const GREP_TOOL_DESCRIPTION = `Search for a text pattern across files.

Searches for literal text (not regex) and returns matching files or content based on output_mode.
Special characters like parentheses, brackets, pipes, etc. are treated as literal characters, not regex operators.

Examples:
- Search all files: \`grep(pattern="TODO")\`
- Search Python files only: \`grep(pattern="import", glob="*.py")\`
- Show matching lines: \`grep(pattern="error", output_mode="content")\`
- Search for code with special chars: \`grep(pattern="def __init__(self):")\``;
const EXECUTE_TOOL_DESCRIPTION = `Executes a shell command in an isolated sandbox environment.

Usage:
Executes a given command in the sandbox environment with proper handling and security measures.
Before executing the command, please follow these steps:

1. Directory Verification:
   - If the command will create new directories or files, first use the ls tool to verify the parent directory exists and is the correct location
   - For example, before running "mkdir foo/bar", first use ls to check that "foo" exists and is the intended parent directory

2. Command Execution:
   - Always quote file paths that contain spaces with double quotes (e.g., cd "path with spaces/file.txt")
   - Examples of proper quoting:
     - cd "/Users/name/My Documents" (correct)
     - cd /Users/name/My Documents (incorrect - will fail)
     - python "/path/with spaces/script.py" (correct)
     - python /path/with spaces/script.py (incorrect - will fail)
   - After ensuring proper quoting, execute the command
   - Capture the output of the command

Usage notes:
  - Commands run in an isolated sandbox environment
  - Returns combined stdout/stderr output with exit code
  - If the output is very large, it may be truncated
  - VERY IMPORTANT: You MUST avoid using search commands like find and grep. Instead use the grep, glob tools to search. You MUST avoid read tools like cat, head, tail, and use read_file to read files.
  - When issuing multiple commands, use the ';' or '&&' operator to separate them. DO NOT use newlines (newlines are ok in quoted strings)
    - Use '&&' when commands depend on each other (e.g., "mkdir dir && cd dir")
    - Use ';' only when you need to run commands sequentially but don't care if earlier commands fail
  - Try to maintain your current working directory throughout the session by using absolute paths and avoiding usage of cd

Examples:
  Good examples:
    - execute(command="pytest /foo/bar/tests")
    - execute(command="python /path/to/script.py")
    - execute(command="npm install && npm test")

  Bad examples (avoid these):
    - execute(command="cd /foo/bar && pytest tests")  # Use absolute path instead
    - execute(command="cat file.txt")  # Use read_file tool instead
    - execute(command="find . -name '*.py'")  # Use glob tool instead
    - execute(command="grep -r 'pattern' .")  # Use grep tool instead

Note: This tool is only available if the backend supports execution (SandboxBackendProtocol).
If execution is not supported, the tool will return an error message.`;
const EXECUTION_SYSTEM_PROMPT = `## Execute Tool \`execute\`

You have access to an \`execute\` tool for running shell commands in a sandboxed environment.
Use this tool to run commands, scripts, tests, builds, and other shell operations.

- execute: run a shell command in the sandbox (returns output and exit code)`;
/**
* Create ls tool using backend.
*/
function createLsTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const path = input.path || "/";
		const infos = await resolvedBackend.lsInfo(path);
		if (infos.length === 0) return `No files found in ${path}`;
		const lines = [];
		for (const info of infos) if (info.is_dir) lines.push(`${info.path} (directory)`);
		else {
			const size = info.size ? ` (${info.size} bytes)` : "";
			lines.push(`${info.path}${size}`);
		}
		return lines.join("\n");
	}, {
		name: "ls",
		description: customDescription || LS_TOOL_DESCRIPTION,
		schema: z.object({ path: z.string().optional().default("/").describe("Directory path to list (default: /)") })
	});
}
/**
* Create read_file tool using backend.
*/
function createReadFileTool(backend, options) {
	const { customDescription, toolTokenLimitBeforeEvict } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const { file_path, offset = DEFAULT_READ_LINE_OFFSET, limit = DEFAULT_READ_LINE_LIMIT } = input;
		let result = await resolvedBackend.read(file_path, offset, limit);
		const lines = result.split("\n");
		if (lines.length > limit) result = lines.slice(0, limit).join("\n");
		if (toolTokenLimitBeforeEvict && result.length >= NUM_CHARS_PER_TOKEN * toolTokenLimitBeforeEvict) {
			const truncationMsg = READ_FILE_TRUNCATION_MSG.replace("{file_path}", file_path);
			const maxContentLength = NUM_CHARS_PER_TOKEN * toolTokenLimitBeforeEvict - truncationMsg.length;
			result = result.substring(0, maxContentLength) + truncationMsg;
		}
		return result;
	}, {
		name: "read_file",
		description: customDescription || READ_FILE_TOOL_DESCRIPTION,
		schema: z.object({
			file_path: z.string().describe("Absolute path to the file to read"),
			offset: z.coerce.number().optional().default(DEFAULT_READ_LINE_OFFSET).describe("Line offset to start reading from (0-indexed)"),
			limit: z.coerce.number().optional().default(DEFAULT_READ_LINE_LIMIT).describe("Maximum number of lines to read")
		})
	});
}
/**
* Create write_file tool using backend.
*/
function createWriteFileTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const { file_path, content } = input;
		const result = await resolvedBackend.write(file_path, content);
		if (result.error) return result.error;
		const message = new ToolMessage({
			content: `Successfully wrote to '${file_path}'`,
			tool_call_id: config.toolCall?.id,
			name: "write_file",
			metadata: result.metadata
		});
		if (result.filesUpdate) return new Command({ update: {
			files: result.filesUpdate,
			messages: [message]
		} });
		return message;
	}, {
		name: "write_file",
		description: customDescription || WRITE_FILE_TOOL_DESCRIPTION,
		schema: z.object({
			file_path: z.string().describe("Absolute path to the file to write"),
			content: z.string().describe("Content to write to the file")
		})
	});
}
/**
* Create edit_file tool using backend.
*/
function createEditFileTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const { file_path, old_string, new_string, replace_all = false } = input;
		const result = await resolvedBackend.edit(file_path, old_string, new_string, replace_all);
		if (result.error) return result.error;
		const message = new ToolMessage({
			content: `Successfully replaced ${result.occurrences} occurrence(s) in '${file_path}'`,
			tool_call_id: config.toolCall?.id,
			name: "edit_file",
			metadata: result.metadata
		});
		if (result.filesUpdate) return new Command({ update: {
			files: result.filesUpdate,
			messages: [message]
		} });
		return message;
	}, {
		name: "edit_file",
		description: customDescription || EDIT_FILE_TOOL_DESCRIPTION,
		schema: z.object({
			file_path: z.string().describe("Absolute path to the file to edit"),
			old_string: z.string().describe("String to be replaced (must match exactly)"),
			new_string: z.string().describe("String to replace with"),
			replace_all: z.boolean().optional().default(false).describe("Whether to replace all occurrences")
		})
	});
}
/**
* Create glob tool using backend.
*/
function createGlobTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const { pattern, path = "/" } = input;
		const infos = await resolvedBackend.globInfo(pattern, path);
		if (infos.length === 0) return `No files found matching pattern '${pattern}'`;
		return infos.map((info) => info.path).join("\n");
	}, {
		name: "glob",
		description: customDescription || GLOB_TOOL_DESCRIPTION,
		schema: z.object({
			pattern: z.string().describe("Glob pattern (e.g., '*.py', '**/*.ts')"),
			path: z.string().optional().default("/").describe("Base path to search from (default: /)")
		})
	});
}
/**
* Create grep tool using backend.
*/
function createGrepTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const { pattern, path = "/", glob = null } = input;
		const result = await resolvedBackend.grepRaw(pattern, path, glob);
		if (typeof result === "string") return result;
		if (result.length === 0) return `No matches found for pattern '${pattern}'`;
		const lines = [];
		let currentFile = null;
		for (const match of result) {
			if (match.path !== currentFile) {
				currentFile = match.path;
				lines.push(`\n${currentFile}:`);
			}
			lines.push(`  ${match.line}: ${match.text}`);
		}
		return lines.join("\n");
	}, {
		name: "grep",
		description: customDescription || GREP_TOOL_DESCRIPTION,
		schema: z.object({
			pattern: z.string().describe("Regex pattern to search for"),
			path: z.string().optional().default("/").describe("Base path to search from (default: /)"),
			glob: z.string().optional().nullable().describe("Optional glob pattern to filter files (e.g., '*.py')")
		})
	});
}
/**
* Create execute tool using backend.
*/
function createExecuteTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		if (!isSandboxBackend(resolvedBackend)) return "Error: Execution not available. This agent's backend does not support command execution (SandboxBackendProtocol). To use the execute tool, provide a backend that implements SandboxBackendProtocol.";
		const result = await resolvedBackend.execute(input.command);
		const parts = [result.output];
		if (result.exitCode !== null) {
			const status = result.exitCode === 0 ? "succeeded" : "failed";
			parts.push(`\n[Command ${status} with exit code ${result.exitCode}]`);
		}
		if (result.truncated) parts.push("\n[Output was truncated due to size limits]");
		return parts.join("");
	}, {
		name: "execute",
		description: customDescription || EXECUTE_TOOL_DESCRIPTION,
		schema: z.object({ command: z.string().describe("The shell command to execute") })
	});
}
/**
* Create filesystem middleware with all tools and features.
*/
function createFilesystemMiddleware(options = {}) {
	const { backend = (stateAndStore) => new StateBackend(stateAndStore), systemPrompt: customSystemPrompt = null, customToolDescriptions = null, toolTokenLimitBeforeEvict = 2e4 } = options;
	const baseSystemPrompt = customSystemPrompt || FILESYSTEM_SYSTEM_PROMPT;
	return createMiddleware({
		name: "FilesystemMiddleware",
		stateSchema: FilesystemStateSchema,
		tools: [
			createLsTool(backend, { customDescription: customToolDescriptions?.ls }),
			createReadFileTool(backend, {
				customDescription: customToolDescriptions?.read_file,
				toolTokenLimitBeforeEvict
			}),
			createWriteFileTool(backend, { customDescription: customToolDescriptions?.write_file }),
			createEditFileTool(backend, { customDescription: customToolDescriptions?.edit_file }),
			createGlobTool(backend, { customDescription: customToolDescriptions?.glob }),
			createGrepTool(backend, { customDescription: customToolDescriptions?.grep }),
			createExecuteTool(backend, { customDescription: customToolDescriptions?.execute })
		],
		wrapModelCall: async (request, handler) => {
			const supportsExecution = isSandboxBackend(getBackend(backend, {
				state: request.state || {},
				store: request.config?.store
			}));
			let tools = request.tools;
			if (!supportsExecution) tools = tools.filter((t) => t.name !== "execute");
			let filesystemPrompt = baseSystemPrompt;
			if (supportsExecution) filesystemPrompt = `${filesystemPrompt}\n\n${EXECUTION_SYSTEM_PROMPT}`;
			const newSystemMessage = request.systemMessage.concat(filesystemPrompt);
			return handler({
				...request,
				tools,
				systemMessage: newSystemMessage
			});
		},
		wrapToolCall: async (request, handler) => {
			if (!toolTokenLimitBeforeEvict) return handler(request);
			const toolName = request.toolCall?.name;
			if (toolName && TOOLS_EXCLUDED_FROM_EVICTION.includes(toolName)) return handler(request);
			const result = await handler(request);
			async function processToolMessage(msg, toolTokenLimitBeforeEvict) {
				if (typeof msg.content === "string" && msg.content.length > toolTokenLimitBeforeEvict * NUM_CHARS_PER_TOKEN) {
					const resolvedBackend = getBackend(backend, {
						state: request.state || {},
						store: request.config?.store
					});
					const evictPath = `/large_tool_results/${sanitizeToolCallId(request.toolCall?.id || msg.tool_call_id)}`;
					const writeResult = await resolvedBackend.write(evictPath, msg.content);
					if (writeResult.error) return {
						message: msg,
						filesUpdate: null
					};
					const contentSample = createContentPreview(msg.content);
					return {
						message: new ToolMessage({
							content: TOO_LARGE_TOOL_MSG.replace("{tool_call_id}", msg.tool_call_id).replace("{file_path}", evictPath).replace("{content_sample}", contentSample),
							tool_call_id: msg.tool_call_id,
							name: msg.name
						}),
						filesUpdate: writeResult.filesUpdate
					};
				}
				return {
					message: msg,
					filesUpdate: null
				};
			}
			if (ToolMessage.isInstance(result)) {
				const processed = await processToolMessage(result, toolTokenLimitBeforeEvict);
				if (processed.filesUpdate) return new Command({ update: {
					files: processed.filesUpdate,
					messages: [processed.message]
				} });
				return processed.message;
			}
			if (isCommand(result)) {
				const update = result.update;
				if (!update?.messages) return result;
				let hasLargeResults = false;
				const accumulatedFiles = update.files ? { ...update.files } : {};
				const processedMessages = [];
				for (const msg of update.messages) if (ToolMessage.isInstance(msg)) {
					const processed = await processToolMessage(msg, toolTokenLimitBeforeEvict);
					processedMessages.push(processed.message);
					if (processed.filesUpdate) {
						hasLargeResults = true;
						Object.assign(accumulatedFiles, processed.filesUpdate);
					}
				} else processedMessages.push(msg);
				if (hasLargeResults) return new Command({ update: {
					...update,
					messages: processedMessages,
					files: accumulatedFiles
				} });
			}
			return result;
		}
	});
}

//#endregion
//#region src/middleware/subagents.ts
/**
* Default system prompt for subagents.
* Provides a minimal base prompt that can be extended by specific subagent configurations.
*/
const DEFAULT_SUBAGENT_PROMPT = "In order to complete the objective that the user asks of you, you have access to a number of standard tools.";
/**
* State keys that are excluded when passing state to subagents and when returning
* updates from subagents.
*
* When returning updates:
* 1. The messages key is handled explicitly to ensure only the final message is included
* 2. The todos and structuredResponse keys are excluded as they do not have a defined reducer
*    and no clear meaning for returning them from a subagent to the main agent.
* 3. The skillsMetadata and memoryContents keys are automatically excluded from subagent output
*    to prevent parent state from leaking to child agents. Each agent loads its own skills/memory
*    independently based on its middleware configuration.
*/
const EXCLUDED_STATE_KEYS = [
	"messages",
	"todos",
	"structuredResponse",
	"skillsMetadata",
	"memoryContents"
];
/**
* Default description for the general-purpose subagent.
* This description is shown to the model when selecting which subagent to use.
*/
const DEFAULT_GENERAL_PURPOSE_DESCRIPTION = "General-purpose agent for researching complex questions, searching for files and content, and executing multi-step tasks. When you are searching for a keyword or file and are not confident that you will find the right match in the first few tries use this agent to perform the search for you. This agent has access to all tools as the main agent.";
function getTaskToolDescription(subagentDescriptions) {
	return `
Launch an ephemeral subagent to handle complex, multi-step independent tasks with isolated context windows.

Available agent types and the tools they have access to:
${subagentDescriptions.join("\n")}

When using the Task tool, you must specify a subagent_type parameter to select which agent type to use.

## Usage notes:
1. Launch multiple agents concurrently whenever possible, to maximize performance; to do that, use a single message with multiple tool uses
2. When the agent is done, it will return a single message back to you. The result returned by the agent is not visible to the user. To show the user the result, you should send a text message back to the user with a concise summary of the result.
3. Each agent invocation is stateless. You will not be able to send additional messages to the agent, nor will the agent be able to communicate with you outside of its final report. Therefore, your prompt should contain a highly detailed task description for the agent to perform autonomously and you should specify exactly what information the agent should return back to you in its final and only message to you.
4. The agent's outputs should generally be trusted
5. Clearly tell the agent whether you expect it to create content, perform analysis, or just do research (search, file reads, web fetches, etc.), since it is not aware of the user's intent
6. If the agent description mentions that it should be used proactively, then you should try your best to use it without the user having to ask for it first. Use your judgement.
7. When only the general-purpose agent is provided, you should use it for all tasks. It is great for isolating context and token usage, and completing specific, complex tasks, as it has all the same capabilities as the main agent.

### Example usage of the general-purpose agent:

<example_agent_descriptions>
"general-purpose": use this agent for general purpose tasks, it has access to all tools as the main agent.
</example_agent_descriptions>

<example>
User: "I want to conduct research on the accomplishments of Lebron James, Michael Jordan, and Kobe Bryant, and then compare them."
Assistant: *Uses the task tool in parallel to conduct isolated research on each of the three players*
Assistant: *Synthesizes the results of the three isolated research tasks and responds to the User*
<commentary>
Research is a complex, multi-step task in it of itself.
The research of each individual player is not dependent on the research of the other players.
The assistant uses the task tool to break down the complex objective into three isolated tasks.
Each research task only needs to worry about context and tokens about one player, then returns synthesized information about each player as the Tool Result.
This means each research task can dive deep and spend tokens and context deeply researching each player, but the final result is synthesized information, and saves us tokens in the long run when comparing the players to each other.
</commentary>
</example>

<example>
User: "Analyze a single large code repository for security vulnerabilities and generate a report."
Assistant: *Launches a single \`task\` subagent for the repository analysis*
Assistant: *Receives report and integrates results into final summary*
<commentary>
Subagent is used to isolate a large, context-heavy task, even though there is only one. This prevents the main thread from being overloaded with details.
If the user then asks followup questions, we have a concise report to reference instead of the entire history of analysis and tool calls, which is good and saves us time and money.
</commentary>
</example>

<example>
User: "Schedule two meetings for me and prepare agendas for each."
Assistant: *Calls the task tool in parallel to launch two \`task\` subagents (one per meeting) to prepare agendas*
Assistant: *Returns final schedules and agendas*
<commentary>
Tasks are simple individually, but subagents help silo agenda preparation.
Each subagent only needs to worry about the agenda for one meeting.
</commentary>
</example>

<example>
User: "I want to order a pizza from Dominos, order a burger from McDonald's, and order a salad from Subway."
Assistant: *Calls tools directly in parallel to order a pizza from Dominos, a burger from McDonald's, and a salad from Subway*
<commentary>
The assistant did not use the task tool because the objective is super simple and clear and only requires a few trivial tool calls.
It is better to just complete the task directly and NOT use the \`task\`tool.
</commentary>
</example>

### Example usage with custom agents:

<example_agent_descriptions>
"content-reviewer": use this agent after you are done creating significant content or documents
"greeting-responder": use this agent when to respond to user greetings with a friendly joke
"research-analyst": use this agent to conduct thorough research on complex topics
</example_agent_description>

<example>
user: "Please write a function that checks if a number is prime"
assistant: Sure let me write a function that checks if a number is prime
assistant: First let me use the Write tool to write a function that checks if a number is prime
assistant: I'm going to use the Write tool to write the following code:
<code>
function isPrime(n) {
  if (n <= 1) return false
  for (let i = 2; i * i <= n; i++) {
    if (n % i === 0) return false
  }
  return true
}
</code>
<commentary>
Since significant content was created and the task was completed, now use the content-reviewer agent to review the work
</commentary>
assistant: Now let me use the content-reviewer agent to review the code
assistant: Uses the Task tool to launch with the content-reviewer agent
</example>

<example>
user: "Can you help me research the environmental impact of different renewable energy sources and create a comprehensive report?"
<commentary>
This is a complex research task that would benefit from using the research-analyst agent to conduct thorough analysis
</commentary>
assistant: I'll help you research the environmental impact of renewable energy sources. Let me use the research-analyst agent to conduct comprehensive research on this topic.
assistant: Uses the Task tool to launch with the research-analyst agent, providing detailed instructions about what research to conduct and what format the report should take
</example>

<example>
user: "Hello"
<commentary>
Since the user is greeting, use the greeting-responder agent to respond with a friendly joke
</commentary>
assistant: "I'm going to use the Task tool to launch with the greeting-responder agent"
</example>
  `.trim();
}
/**
* System prompt section that explains how to use the task tool for spawning subagents.
*
* This prompt is automatically appended to the main agent's system prompt when
* using `createSubAgentMiddleware`. It provides guidance on:
* - When to use the task tool
* - Subagent lifecycle (spawn  run  return  reconcile)
* - When NOT to use the task tool
* - Best practices for parallel task execution
*
* You can provide a custom `systemPrompt` to `createSubAgentMiddleware` to override
* or extend this default.
*/
const TASK_SYSTEM_PROMPT = `## \`task\` (subagent spawner)

You have access to a \`task\` tool to launch short-lived subagents that handle isolated tasks. These agents are ephemeral  they live only for the duration of the task and return a single result.

When to use the task tool:
- When a task is complex and multi-step, and can be fully delegated in isolation
- When a task is independent of other tasks and can run in parallel
- When a task requires focused reasoning or heavy token/context usage that would bloat the orchestrator thread
- When sandboxing improves reliability (e.g. code execution, structured searches, data formatting)
- When you only care about the output of the subagent, and not the intermediate steps (ex. performing a lot of research and then returned a synthesized report, performing a series of computations or lookups to achieve a concise, relevant answer.)

Subagent lifecycle:
1. **Spawn**  Provide clear role, instructions, and expected output
2. **Run**  The subagent completes the task autonomously
3. **Return**  The subagent provides a single structured result
4. **Reconcile**  Incorporate or synthesize the result into the main thread

When NOT to use the task tool:
- If you need to see the intermediate reasoning or steps after the subagent has completed (the task tool hides them)
- If the task is trivial (a few tool calls or simple lookup)
- If delegating does not reduce token usage, complexity, or context switching
- If splitting would add latency without benefit

## Important Task Tool Usage Notes to Remember
- Whenever possible, parallelize the work that you do. This is true for both tool_calls, and for tasks. Whenever you have independent steps to complete - make tool_calls, or kick off tasks (subagents) in parallel to accomplish them faster. This saves time for the user, which is incredibly important.
- Remember to use the \`task\` tool to silo independent tasks within a multi-part objective.
- You should use the \`task\` tool whenever you have a complex task that will take multiple steps, and is independent from other tasks that the agent needs to complete. These agents are highly competent and efficient.`;
/**
* Base specification for the general-purpose subagent.
*
* This constant provides the default configuration for the general-purpose subagent
* that is automatically included when `generalPurposeAgent: true` (the default).
*
* The general-purpose subagent:
* - Has access to all tools from the main agent
* - Inherits skills from the main agent (when skills are configured)
* - Uses the same model as the main agent (by default)
* - Is ideal for delegating complex, multi-step tasks
*
* You can spread this constant and override specific properties when creating
* custom subagents that should behave similarly to the general-purpose agent:
*
* @example
* ```typescript
* import { GENERAL_PURPOSE_SUBAGENT, createDeepAgent } from "@anthropic/deepagents";
*
* // Use as-is (automatically included with generalPurposeAgent: true)
* const agent = createDeepAgent({ model: "claude-sonnet-4-5-20250929" });
*
* // Or create a custom variant with different tools
* const customGP: SubAgent = {
*   ...GENERAL_PURPOSE_SUBAGENT,
*   name: "research-gp",
*   tools: [webSearchTool, readFileTool],
* };
*
* const agent = createDeepAgent({
*   model: "claude-sonnet-4-5-20250929",
*   subagents: [customGP],
*   // Disable the default general-purpose agent since we're providing our own
*   // (handled automatically when using createSubAgentMiddleware directly)
* });
* ```
*/
const GENERAL_PURPOSE_SUBAGENT = {
	name: "general-purpose",
	description: DEFAULT_GENERAL_PURPOSE_DESCRIPTION,
	systemPrompt: DEFAULT_SUBAGENT_PROMPT
};
/**
* Filter state to exclude certain keys when passing to subagents
*/
function filterStateForSubagent(state) {
	const filtered = {};
	for (const [key, value] of Object.entries(state)) if (!EXCLUDED_STATE_KEYS.includes(key)) filtered[key] = value;
	return filtered;
}
/**
* Create Command with filtered state update from subagent result
*/
function returnCommandWithStateUpdate(result, toolCallId) {
	const stateUpdate = filterStateForSubagent(result);
	const messages = result.messages;
	const lastMessage = messages?.[messages.length - 1];
	return new Command({ update: {
		...stateUpdate,
		messages: [new ToolMessage({
			content: lastMessage?.content || "Task completed",
			tool_call_id: toolCallId,
			name: "task"
		})]
	} });
}
/**
* Create subagent instances from specifications
*/
function getSubagents(options) {
	const { defaultModel, defaultTools, defaultMiddleware, generalPurposeMiddleware: gpMiddleware, defaultInterruptOn, subagents, generalPurposeAgent } = options;
	const defaultSubagentMiddleware = defaultMiddleware || [];
	const generalPurposeMiddlewareBase = gpMiddleware || defaultSubagentMiddleware;
	const agents = {};
	const subagentDescriptions = [];
	if (generalPurposeAgent) {
		const generalPurposeMiddleware = [...generalPurposeMiddlewareBase];
		if (defaultInterruptOn) generalPurposeMiddleware.push(humanInTheLoopMiddleware({ interruptOn: defaultInterruptOn }));
		agents["general-purpose"] = createAgent({
			model: defaultModel,
			systemPrompt: DEFAULT_SUBAGENT_PROMPT,
			tools: defaultTools,
			middleware: generalPurposeMiddleware,
			name: "general-purpose"
		});
		subagentDescriptions.push(`- general-purpose: ${DEFAULT_GENERAL_PURPOSE_DESCRIPTION}`);
	}
	for (const agentParams of subagents) {
		subagentDescriptions.push(`- ${agentParams.name}: ${agentParams.description}`);
		if ("runnable" in agentParams) agents[agentParams.name] = agentParams.runnable;
		else {
			const middleware = agentParams.middleware ? [...defaultSubagentMiddleware, ...agentParams.middleware] : [...defaultSubagentMiddleware];
			const interruptOn = agentParams.interruptOn || defaultInterruptOn;
			if (interruptOn) middleware.push(humanInTheLoopMiddleware({ interruptOn }));
			agents[agentParams.name] = createAgent({
				model: agentParams.model ?? defaultModel,
				systemPrompt: agentParams.systemPrompt,
				tools: agentParams.tools ?? defaultTools,
				middleware,
				name: agentParams.name
			});
		}
	}
	return {
		agents,
		descriptions: subagentDescriptions
	};
}
/**
* Create the task tool for invoking subagents
*/
function createTaskTool(options) {
	const { defaultModel, defaultTools, defaultMiddleware, generalPurposeMiddleware, defaultInterruptOn, subagents, generalPurposeAgent, taskDescription } = options;
	const { agents: subagentGraphs, descriptions: subagentDescriptions } = getSubagents({
		defaultModel,
		defaultTools,
		defaultMiddleware,
		generalPurposeMiddleware,
		defaultInterruptOn,
		subagents,
		generalPurposeAgent
	});
	return tool(async (input, config) => {
		const { description, subagent_type } = input;
		if (!(subagent_type in subagentGraphs)) {
			const allowedTypes = Object.keys(subagentGraphs).map((k) => `\`${k}\``).join(", ");
			throw new Error(`Error: invoked agent of type ${subagent_type}, the only allowed types are ${allowedTypes}`);
		}
		const subagent = subagentGraphs[subagent_type];
		const subagentState = filterStateForSubagent(getCurrentTaskInput());
		subagentState.messages = [new HumanMessage$1({ content: description })];
		const result = await subagent.invoke(subagentState, config);
		if (!config.toolCall?.id) throw new Error("Tool call ID is required for subagent invocation");
		return returnCommandWithStateUpdate(result, config.toolCall.id);
	}, {
		name: "task",
		description: taskDescription ? taskDescription : getTaskToolDescription(subagentDescriptions),
		schema: z.object({
			description: z.string().describe("The task to execute with the selected agent"),
			subagent_type: z.string().describe(`Name of the agent to use. Available: ${Object.keys(subagentGraphs).join(", ")}`)
		})
	});
}
/**
* Create subagent middleware with task tool
*/
function createSubAgentMiddleware(options) {
	const { defaultModel, defaultTools = [], defaultMiddleware = null, generalPurposeMiddleware = null, defaultInterruptOn = null, subagents = [], systemPrompt = TASK_SYSTEM_PROMPT, generalPurposeAgent = true, taskDescription = null } = options;
	return createMiddleware({
		name: "subAgentMiddleware",
		tools: [createTaskTool({
			defaultModel,
			defaultTools,
			defaultMiddleware,
			generalPurposeMiddleware,
			defaultInterruptOn,
			subagents,
			generalPurposeAgent,
			taskDescription
		})],
		wrapModelCall: async (request, handler) => {
			if (systemPrompt !== null) return handler({
				...request,
				systemMessage: request.systemMessage.concat(new SystemMessage({ content: systemPrompt }))
			});
			return handler(request);
		}
	});
}

//#endregion
//#region src/middleware/patch_tool_calls.ts
/**
* Patch dangling tool calls in a messages array.
* Returns the patched messages array and a flag indicating if patching was needed.
*
* @param messages - The messages array to patch
* @returns Object with patched messages and needsPatch flag
*/
function patchDanglingToolCalls(messages) {
	if (!messages || messages.length === 0) return {
		patchedMessages: [],
		needsPatch: false
	};
	const patchedMessages = [];
	let needsPatch = false;
	for (let i = 0; i < messages.length; i++) {
		const msg = messages[i];
		patchedMessages.push(msg);
		if (AIMessage.isInstance(msg) && msg.tool_calls != null) {
			for (const toolCall of msg.tool_calls) if (!messages.slice(i).find((m) => ToolMessage.isInstance(m) && m.tool_call_id === toolCall.id)) {
				needsPatch = true;
				const toolMsg = `Tool call ${toolCall.name} with id ${toolCall.id} was cancelled - another message came in before it could be completed.`;
				patchedMessages.push(new ToolMessage({
					content: toolMsg,
					name: toolCall.name,
					tool_call_id: toolCall.id
				}));
			}
		}
	}
	return {
		patchedMessages,
		needsPatch
	};
}
/**
* Create middleware that patches dangling tool calls in the messages history.
*
* When an AI message contains tool_calls but subsequent messages don't include
* the corresponding ToolMessage responses, this middleware adds synthetic
* ToolMessages saying the tool call was cancelled.
*
* This middleware patches in two places:
* 1. `beforeAgent`: Patches state at the start of the agent loop (handles most cases)
* 2. `wrapModelCall`: Patches the request right before model invocation (handles
*    edge cases like HITL rejection during graph resume where state updates from
*    beforeAgent may not be applied in time)
*
* @returns AgentMiddleware that patches dangling tool calls
*
* @example
* ```typescript
* import { createAgent } from "langchain";
* import { createPatchToolCallsMiddleware } from "./middleware/patch_tool_calls";
*
* const agent = createAgent({
*   model: "claude-sonnet-4-5-20250929",
*   middleware: [createPatchToolCallsMiddleware()],
* });
* ```
*/
function createPatchToolCallsMiddleware() {
	return createMiddleware({
		name: "patchToolCallsMiddleware",
		beforeAgent: async (state) => {
			const messages = state.messages;
			if (!messages || messages.length === 0) return;
			const { patchedMessages, needsPatch } = patchDanglingToolCalls(messages);
			/**
			* Only trigger REMOVE_ALL_MESSAGES if patching is actually needed
			*/
			if (!needsPatch) return;
			return { messages: [new RemoveMessage({ id: REMOVE_ALL_MESSAGES }), ...patchedMessages] };
		},
		wrapModelCall: async (request, handler) => {
			const messages = request.messages;
			if (!messages || messages.length === 0) return handler(request);
			const { patchedMessages, needsPatch } = patchDanglingToolCalls(messages);
			if (!needsPatch) return handler(request);
			return handler({
				...request,
				messages: patchedMessages
			});
		}
	});
}

//#endregion
//#region src/values.ts
/**
* Shared state values for use in StateSchema definitions.
*
* This module provides pre-configured ReducedValue instances that can be
* reused across different state schemas, similar to LangGraph's messagesValue.
*/
/**
* Shared ReducedValue for file data state management.
*
* This provides a reusable pattern for managing file state with automatic
* merging of concurrent updates from parallel subagents. Files can be updated
* or deleted (using null values) and the reducer handles the merge logic.
*
* Similar to LangGraph's messagesValue, this encapsulates the common pattern
* of managing files in agent state so you don't have to manually configure
* the ReducedValue each time.
*
* @example
* ```typescript
* import { filesValue } from "@anthropic/deepagents";
* import { StateSchema } from "@langchain/langgraph";
*
* const MyStateSchema = new StateSchema({
*   files: filesValue,
*   // ... other state fields
* });
* ```
*/
const filesValue = new ReducedValue(z$1.record(z$1.string(), FileDataSchema).default(() => ({})), {
	inputSchema: z$1.record(z$1.string(), FileDataSchema.nullable()).optional(),
	reducer: fileDataReducer
});

//#endregion
//#region src/middleware/memory.ts
/**
* Middleware for loading agent memory/context from AGENTS.md files.
*
* This module implements support for the AGENTS.md specification (https://agents.md/),
* loading memory/context from configurable sources and injecting into the system prompt.
*
* ## Overview
*
* AGENTS.md files provide project-specific context and instructions to help AI agents
* work effectively. Unlike skills (which are on-demand workflows), memory is always
* loaded and provides persistent context.
*
* ## Usage
*
* ```typescript
* import { createMemoryMiddleware } from "@anthropic/deepagents";
* import { FilesystemBackend } from "@anthropic/deepagents";
*
* // Security: FilesystemBackend allows reading/writing from the entire filesystem.
* // Either ensure the agent is running within a sandbox OR add human-in-the-loop (HIL)
* // approval to file operations.
* const backend = new FilesystemBackend({ rootDir: "/" });
*
* const middleware = createMemoryMiddleware({
*   backend,
*   sources: [
*     "~/.deepagents/AGENTS.md",
*     "./.deepagents/AGENTS.md",
*   ],
* });
*
* const agent = createDeepAgent({ middleware: [middleware] });
* ```
*
* ## Memory Sources
*
* Sources are simply paths to AGENTS.md files that are loaded in order and combined.
* Multiple sources are concatenated in order, with all content included.
* Later sources appear after earlier ones in the combined prompt.
*
* ## File Format
*
* AGENTS.md files are standard Markdown with no required structure.
* Common sections include:
* - Project overview
* - Build/test commands
* - Code style guidelines
* - Architecture notes
*/
/**
* State schema for memory middleware.
*/
const MemoryStateSchema = new StateSchema({
	memoryContents: z$1.record(z$1.string(), z$1.string()).optional(),
	files: filesValue
});
/**
* Default system prompt template for memory.
* Ported from Python's comprehensive memory guidelines.
*/
const MEMORY_SYSTEM_PROMPT = `<agent_memory>
{memory_contents}
</agent_memory>

<memory_guidelines>
    The above <agent_memory> was loaded in from files in your filesystem. As you learn from your interactions with the user, you can save new knowledge by calling the \`edit_file\` tool.

    **Learning from feedback:**
    - One of your MAIN PRIORITIES is to learn from your interactions with the user. These learnings can be implicit or explicit. This means that in the future, you will remember this important information.
    - When you need to remember something, updating memory must be your FIRST, IMMEDIATE action - before responding to the user, before calling other tools, before doing anything else. Just update memory immediately.
    - When user says something is better/worse, capture WHY and encode it as a pattern.
    - Each correction is a chance to improve permanently - don't just fix the immediate issue, update your instructions.
    - A great opportunity to update your memories is when the user interrupts a tool call and provides feedback. You should update your memories immediately before revising the tool call.
    - Look for the underlying principle behind corrections, not just the specific mistake.
    - The user might not explicitly ask you to remember something, but if they provide information that is useful for future use, you should update your memories immediately.

    **Asking for information:**
    - If you lack context to perform an action (e.g. send a Slack DM, requires a user ID/email) you should explicitly ask the user for this information.
    - It is preferred for you to ask for information, don't assume anything that you do not know!
    - When the user provides information that is useful for future use, you should update your memories immediately.

    **When to update memories:**
    - When the user explicitly asks you to remember something (e.g., "remember my email", "save this preference")
    - When the user describes your role or how you should behave (e.g., "you are a web researcher", "always do X")
    - When the user gives feedback on your work - capture what was wrong and how to improve
    - When the user provides information required for tool use (e.g., slack channel ID, email addresses)
    - When the user provides context useful for future tasks, such as how to use tools, or which actions to take in a particular situation
    - When you discover new patterns or preferences (coding styles, conventions, workflows)

    **When to NOT update memories:**
    - When the information is temporary or transient (e.g., "I'm running late", "I'm on my phone right now")
    - When the information is a one-time task request (e.g., "Find me a recipe", "What's 25 * 4?")
    - When the information is a simple question that doesn't reveal lasting preferences (e.g., "What day is it?", "Can you explain X?")
    - When the information is an acknowledgment or small talk (e.g., "Sounds good!", "Hello", "Thanks for that")
    - When the information is stale or irrelevant in future conversations
    - Never store API keys, access tokens, passwords, or any other credentials in any file, memory, or system prompt.
    - If the user asks where to put API keys or provides an API key, do NOT echo or save it.

    **Examples:**
    Example 1 (remembering user information):
    User: Can you connect to my google account?
    Agent: Sure, I'll connect to your google account, what's your google account email?
    User: john@example.com
    Agent: Let me save this to my memory.
    Tool Call: edit_file(...) -> remembers that the user's google account email is john@example.com

    Example 2 (remembering implicit user preferences):
    User: Can you write me an example for creating a deep agent in LangChain?
    Agent: Sure, I'll write you an example for creating a deep agent in LangChain <example code in Python>
    User: Can you do this in JavaScript
    Agent: Let me save this to my memory.
    Tool Call: edit_file(...) -> remembers that the user prefers to get LangChain code examples in JavaScript
    Agent: Sure, here is the JavaScript example<example code in JavaScript>

    Example 3 (do not remember transient information):
    User: I'm going to play basketball tonight so I will be offline for a few hours.
    Agent: Okay I'll add a block to your calendar.
    Tool Call: create_calendar_event(...) -> just calls a tool, does not commit anything to memory, as it is transient information
</memory_guidelines>`;
/**
* Format loaded memory contents for injection into prompt.
* Pairs memory locations with their contents for clarity.
*/
function formatMemoryContents(contents, sources) {
	if (Object.keys(contents).length === 0) return "(No memory loaded)";
	const sections = [];
	for (const path of sources) if (contents[path]) sections.push(`${path}\n${contents[path]}`);
	if (sections.length === 0) return "(No memory loaded)";
	return sections.join("\n\n");
}
/**
* Load memory content from a backend path.
*
* @param backend - Backend to load from.
* @param path - Path to the AGENTS.md file.
* @returns File content if found, null otherwise.
*/
async function loadMemoryFromBackend(backend, path) {
	if (!backend.downloadFiles) {
		const content = await backend.read(path);
		if (content.startsWith("Error:")) return null;
		return content;
	}
	const results = await backend.downloadFiles([path]);
	if (results.length !== 1) throw new Error(`Expected 1 response for path ${path}, got ${results.length}`);
	const response = results[0];
	if (response.error != null) {
		if (response.error === "file_not_found") return null;
		throw new Error(`Failed to download ${path}: ${response.error}`);
	}
	if (response.content != null) return new TextDecoder().decode(response.content);
	return null;
}
/**
* Create middleware for loading agent memory from AGENTS.md files.
*
* Loads memory content from configured sources and injects into the system prompt.
* Supports multiple sources that are combined together.
*
* @param options - Configuration options
* @returns AgentMiddleware for memory loading and injection
*
* @example
* ```typescript
* const middleware = createMemoryMiddleware({
*   backend: new FilesystemBackend({ rootDir: "/" }),
*   sources: [
*     "~/.deepagents/AGENTS.md",
*     "./.deepagents/AGENTS.md",
*   ],
* });
* ```
*/
function createMemoryMiddleware(options) {
	const { backend, sources } = options;
	/**
	* Resolve backend from instance or factory.
	*/
	function getBackend(state) {
		if (typeof backend === "function") return backend({ state });
		return backend;
	}
	return createMiddleware({
		name: "MemoryMiddleware",
		stateSchema: MemoryStateSchema,
		async beforeAgent(state) {
			if ("memoryContents" in state && state.memoryContents != null) return;
			const resolvedBackend = getBackend(state);
			const contents = {};
			for (const path of sources) try {
				const content = await loadMemoryFromBackend(resolvedBackend, path);
				if (content) contents[path] = content;
			} catch (error) {
				console.debug(`Failed to load memory from ${path}:`, error);
			}
			return { memoryContents: contents };
		},
		wrapModelCall(request, handler) {
			const formattedContents = formatMemoryContents(request.state?.memoryContents || {}, sources);
			const newSystemMessage = new SystemMessage(MEMORY_SYSTEM_PROMPT.replace("{memory_contents}", formattedContents)).concat(request.systemMessage);
			return handler({
				...request,
				systemMessage: newSystemMessage
			});
		}
	});
}

//#endregion
//#region src/middleware/skills.ts
/**
* Backend-agnostic skills middleware for loading agent skills from any backend.
*
* This middleware implements Anthropic's agent skills pattern with progressive disclosure,
* loading skills from backend storage via configurable sources.
*
* ## Architecture
*
* Skills are loaded from one or more **sources** - paths in a backend where skills are
* organized. Sources are loaded in order, with later sources overriding earlier ones
* when skills have the same name (last one wins). This enables layering: base -> user
* -> project -> team skills.
*
* The middleware uses backend APIs exclusively (no direct filesystem access), making it
* portable across different storage backends (filesystem, state, remote storage, etc.).
*
* ## Usage
*
* ```typescript
* import { createSkillsMiddleware, FilesystemBackend } from "@anthropic/deepagents";
*
* const middleware = createSkillsMiddleware({
*   backend: new FilesystemBackend({ rootDir: "/" }),
*   sources: [
*     "/skills/user/",
*     "/skills/project/",
*   ],
* });
*
* const agent = createDeepAgent({ middleware: [middleware] });
* ```
*
* Or use the `skills` parameter on createDeepAgent:
*
* ```typescript
* const agent = createDeepAgent({
*   skills: ["/skills/user/", "/skills/project/"],
* });
* ```
*/
const MAX_SKILL_FILE_SIZE = 10 * 1024 * 1024;
const MAX_SKILL_NAME_LENGTH = 64;
const MAX_SKILL_DESCRIPTION_LENGTH = 1024;
const MAX_SKILL_COMPATIBILITY_LENGTH = 500;
/**
* Zod schema for a single skill metadata entry.
*/
const SkillMetadataEntrySchema = z$1.object({
	name: z$1.string(),
	description: z$1.string(),
	path: z$1.string(),
	license: z$1.string().nullable().optional(),
	compatibility: z$1.string().nullable().optional(),
	metadata: z$1.record(z$1.string(), z$1.string()).optional(),
	allowedTools: z$1.array(z$1.string()).optional()
});
/**
* Reducer for skillsMetadata that merges arrays from parallel subagents.
* Skills are deduplicated by name, with later values overriding earlier ones.
*
* @param current - The current skillsMetadata array (from state)
* @param update - The new skillsMetadata array (from a subagent update)
* @returns Merged array with duplicates resolved by name (later values win)
*/
function skillsMetadataReducer(current, update) {
	if (!update || update.length === 0) return current || [];
	if (!current || current.length === 0) return update;
	const merged = /* @__PURE__ */ new Map();
	for (const skill of current) merged.set(skill.name, skill);
	for (const skill of update) merged.set(skill.name, skill);
	return Array.from(merged.values());
}
/**
* State schema for skills middleware.
* Uses ReducedValue for skillsMetadata to allow concurrent updates from parallel subagents.
*/
const SkillsStateSchema = new StateSchema({
	skillsMetadata: new ReducedValue(z$1.array(SkillMetadataEntrySchema).default(() => []), {
		inputSchema: z$1.array(SkillMetadataEntrySchema).optional(),
		reducer: skillsMetadataReducer
	}),
	files: filesValue
});
/**
* Skills System Documentation prompt template.
*/
const SKILLS_SYSTEM_PROMPT = `
## Skills System

You have access to a skills library that provides specialized capabilities and domain knowledge.

{skills_locations}

**Available Skills:**

{skills_list}

**How to Use Skills (Progressive Disclosure):**

Skills follow a **progressive disclosure** pattern - you know they exist (name + description above), but you only read the full instructions when needed:

1. **Recognize when a skill applies**: Check if the user's task matches any skill's description
2. **Read the skill's full instructions**: The skill list above shows the exact path to use with read_file
3. **Follow the skill's instructions**: SKILL.md contains step-by-step workflows, best practices, and examples
4. **Access supporting files**: Skills may include Python scripts, configs, or reference docs - use absolute paths

**When to Use Skills:**
- When the user's request matches a skill's domain (e.g., "research X"  web-research skill)
- When you need specialized knowledge or structured workflows
- When a skill provides proven patterns for complex tasks

**Skills are Self-Documenting:**
- Each SKILL.md tells you exactly what the skill does and how to use it
- The skill list above shows the full path for each skill's SKILL.md file

**Executing Skill Scripts:**
Skills may contain Python scripts or other executable files. Always use absolute paths from the skill list.

**Example Workflow:**

User: "Can you research the latest developments in quantum computing?"

1. Check available skills above  See "web-research" skill with its full path
2. Read the skill using the path shown in the list
3. Follow the skill's research workflow (search  organize  synthesize)
4. Use any helper scripts with absolute paths

Remember: Skills are tools to make you more capable and consistent. When in doubt, check if a skill exists for the task!
`;
/**
* Validate skill name per Agent Skills specification.
*
* Constraints per Agent Skills specification:
*
* - 1-64 characters
* - Unicode lowercase alphanumeric and hyphens only (`a-z` and `-`).
* - Must not start or end with `-`
* - Must not contain consecutive `--`
* - Must match the parent directory name containing the `SKILL.md` file
*
* Unicode lowercase alphanumeric means any lowercase or decimal digit, which
* covers accented Latin characters (e.g., `'caf'`, `'ber-tool'`) and other
* scripts.
*
* @param name - The skill name from YAML frontmatter
* @param directoryName - The parent directory name
* @returns `{ valid, error }` tuple. Error is empty string if valid.
*/
function validateSkillName$1(name, directoryName) {
	if (!name) return {
		valid: false,
		error: "name is required"
	};
	if (name.length > MAX_SKILL_NAME_LENGTH) return {
		valid: false,
		error: "name exceeds 64 characters"
	};
	if (name.startsWith("-") || name.endsWith("-") || name.includes("--")) return {
		valid: false,
		error: "name must be lowercase alphanumeric with single hyphens only"
	};
	for (const c of name) {
		if (c === "-") continue;
		if (/\p{Ll}/u.test(c) || /\p{Nd}/u.test(c)) continue;
		return {
			valid: false,
			error: "name must be lowercase alphanumeric with single hyphens only"
		};
	}
	if (name !== directoryName) return {
		valid: false,
		error: `name '${name}' must match directory name '${directoryName}'`
	};
	return {
		valid: true,
		error: ""
	};
}
/**
* Validate and normalize the metadata field from YAML frontmatter.
*
* YAML parsing can return any type for the `metadata` key. This ensures the
* value in {@link SkillMetadata} is always a `Record<string, string>` by
* coercing via `String()` and rejecting non-object inputs.
*
* @param raw - Raw value from `frontmatterData.metadata`.
* @param skillPath - Path to the `SKILL.md` file (for warning messages).
* @returns A validated `Record<string, string>`.
*/
function validateMetadata(raw, skillPath) {
	if (typeof raw !== "object" || raw === null || Array.isArray(raw)) {
		if (raw) console.warn(`Ignoring non-object metadata in ${skillPath} (got ${typeof raw})`);
		return {};
	}
	const result = {};
	for (const [k, v] of Object.entries(raw)) result[String(k)] = String(v);
	return result;
}
/**
* Build a parenthetical annotation string from optional skill fields.
*
* Combines license and compatibility into a comma-separated string for
* display in the system prompt skill listing.
*
* @param skill - Skill metadata to extract annotations from.
* @returns Annotation string like `'License: MIT, Compatibility: Python 3.10+'`,
*   or empty string if neither field is set.
*/
function formatSkillAnnotations(skill) {
	const parts = [];
	if (skill.license) parts.push(`License: ${skill.license}`);
	if (skill.compatibility) parts.push(`Compatibility: ${skill.compatibility}`);
	return parts.join(", ");
}
/**
* Parse YAML frontmatter from `SKILL.md` content.
*
* Extracts metadata per Agent Skills specification from YAML frontmatter
* delimited by `---` markers at the start of the content.
*
* @param content - Content of the `SKILL.md` file
* @param skillPath - Path to the `SKILL.md` file (for error messages and metadata)
* @param directoryName - Name of the parent directory containing the skill
* @returns `SkillMetadata` if parsing succeeds, `null` if parsing fails or
*   validation errors occur
*/
function parseSkillMetadataFromContent(content, skillPath, directoryName) {
	if (content.length > MAX_SKILL_FILE_SIZE) {
		console.warn(`Skipping ${skillPath}: content too large (${content.length} bytes)`);
		return null;
	}
	const match = content.match(/^---\s*\n([\s\S]*?)\n---\s*\n/);
	if (!match) {
		console.warn(`Skipping ${skillPath}: no valid YAML frontmatter found`);
		return null;
	}
	const frontmatterStr = match[1];
	let frontmatterData;
	try {
		frontmatterData = yaml.parse(frontmatterStr);
	} catch (e) {
		console.warn(`Invalid YAML in ${skillPath}:`, e);
		return null;
	}
	if (!frontmatterData || typeof frontmatterData !== "object") {
		console.warn(`Skipping ${skillPath}: frontmatter is not a mapping`);
		return null;
	}
	const name = String(frontmatterData.name ?? "").trim();
	const description = String(frontmatterData.description ?? "").trim();
	if (!name || !description) {
		console.warn(`Skipping ${skillPath}: missing required 'name' or 'description'`);
		return null;
	}
	const validation = validateSkillName$1(name, directoryName);
	if (!validation.valid) console.warn(`Skill '${name}' in ${skillPath} does not follow Agent Skills specification: ${validation.error}. Consider renaming for spec compliance.`);
	let descriptionStr = description;
	if (descriptionStr.length > MAX_SKILL_DESCRIPTION_LENGTH) {
		console.warn(`Description exceeds ${MAX_SKILL_DESCRIPTION_LENGTH} characters in ${skillPath}, truncating`);
		descriptionStr = descriptionStr.slice(0, MAX_SKILL_DESCRIPTION_LENGTH);
	}
	const rawTools = frontmatterData["allowed-tools"];
	let allowedTools;
	if (rawTools) if (Array.isArray(rawTools)) allowedTools = rawTools.map((t) => String(t).trim()).filter(Boolean);
	else allowedTools = String(rawTools).split(/\s+/).filter(Boolean);
	else allowedTools = [];
	let compatibilityStr = String(frontmatterData.compatibility ?? "").trim() || null;
	if (compatibilityStr && compatibilityStr.length > MAX_SKILL_COMPATIBILITY_LENGTH) {
		console.warn(`Compatibility exceeds ${MAX_SKILL_COMPATIBILITY_LENGTH} characters in ${skillPath}, truncating`);
		compatibilityStr = compatibilityStr.slice(0, MAX_SKILL_COMPATIBILITY_LENGTH);
	}
	return {
		name,
		description: descriptionStr,
		path: skillPath,
		metadata: validateMetadata(frontmatterData.metadata ?? {}, skillPath),
		license: String(frontmatterData.license ?? "").trim() || null,
		compatibility: compatibilityStr,
		allowedTools
	};
}
/**
* List all skills from a backend source.
*/
async function listSkillsFromBackend(backend, sourcePath) {
	const skills = [];
	const pathSep = sourcePath.includes("\\") ? "\\" : "/";
	const normalizedPath = sourcePath.endsWith("/") || sourcePath.endsWith("\\") ? sourcePath : `${sourcePath}${pathSep}`;
	let fileInfos;
	try {
		fileInfos = await backend.lsInfo(normalizedPath);
	} catch {
		return [];
	}
	const entries = fileInfos.map((info) => ({
		name: info.path.replace(/[/\\]$/, "").split(/[/\\]/).pop() || "",
		type: info.is_dir ? "directory" : "file"
	}));
	for (const entry of entries) {
		if (entry.type !== "directory") continue;
		const skillMdPath = `${normalizedPath}${entry.name}${pathSep}SKILL.md`;
		let content;
		if (backend.downloadFiles) {
			const results = await backend.downloadFiles([skillMdPath]);
			if (results.length !== 1) continue;
			const response = results[0];
			if (response.error != null || response.content == null) continue;
			content = new TextDecoder().decode(response.content);
		} else {
			const readResult = await backend.read(skillMdPath);
			if (readResult.startsWith("Error:")) continue;
			content = readResult;
		}
		const metadata = parseSkillMetadataFromContent(content, skillMdPath, entry.name);
		if (metadata) skills.push(metadata);
	}
	return skills;
}
/**
* Format skills locations for display in system prompt.
* Shows priority indicator for the last source (highest priority).
*/
function formatSkillsLocations(sources) {
	if (sources.length === 0) return "**Skills Sources:** None configured";
	const lines = [];
	for (let i = 0; i < sources.length; i++) {
		const sourcePath = sources[i];
		const name = sourcePath.replace(/[/\\]$/, "").split(/[/\\]/).filter(Boolean).pop()?.replace(/^./, (c) => c.toUpperCase()) || "Skills";
		const suffix = i === sources.length - 1 ? " (higher priority)" : "";
		lines.push(`**${name} Skills**: \`${sourcePath}\`${suffix}`);
	}
	return lines.join("\n");
}
/**
* Format skills metadata for display in system prompt.
* Shows allowed tools for each skill if specified.
*/
function formatSkillsList(skills, sources) {
	if (skills.length === 0) return `(No skills available yet. You can create skills in ${sources.map((s) => `\`${s}\``).join(" or ")})`;
	const lines = [];
	for (const skill of skills) {
		const annotations = formatSkillAnnotations(skill);
		let descLine = `- **${skill.name}**: ${skill.description}`;
		if (annotations) descLine += ` (${annotations})`;
		lines.push(descLine);
		if (skill.allowedTools && skill.allowedTools.length > 0) lines.push(`   Allowed tools: ${skill.allowedTools.join(", ")}`);
		lines.push(`   Read \`${skill.path}\` for full instructions`);
	}
	return lines.join("\n");
}
/**
* Create backend-agnostic middleware for loading and exposing agent skills.
*
* This middleware loads skills from configurable backend sources and injects
* skill metadata into the system prompt. It implements the progressive disclosure
* pattern: skill names and descriptions are shown in the prompt, but the agent
* reads full SKILL.md content only when needed.
*
* @param options - Configuration options
* @returns AgentMiddleware for skills loading and injection
*
* @example
* ```typescript
* const middleware = createSkillsMiddleware({
*   backend: new FilesystemBackend({ rootDir: "/" }),
*   sources: ["/skills/user/", "/skills/project/"],
* });
* ```
*/
function createSkillsMiddleware(options) {
	const { backend, sources } = options;
	let loadedSkills = [];
	/**
	* Resolve backend from instance or factory.
	*/
	function getBackend(state) {
		if (typeof backend === "function") return backend({ state });
		return backend;
	}
	return createMiddleware({
		name: "SkillsMiddleware",
		stateSchema: SkillsStateSchema,
		async beforeAgent(state) {
			if (loadedSkills.length > 0) return;
			if ("skillsMetadata" in state && Array.isArray(state.skillsMetadata) && state.skillsMetadata.length > 0) {
				loadedSkills = state.skillsMetadata;
				return;
			}
			const resolvedBackend = getBackend(state);
			const allSkills = /* @__PURE__ */ new Map();
			for (const sourcePath of sources) try {
				const skills = await listSkillsFromBackend(resolvedBackend, sourcePath);
				for (const skill of skills) allSkills.set(skill.name, skill);
			} catch (error) {
				console.debug(`[BackendSkillsMiddleware] Failed to load skills from ${sourcePath}:`, error);
			}
			loadedSkills = Array.from(allSkills.values());
			return { skillsMetadata: loadedSkills };
		},
		wrapModelCall(request, handler) {
			const skillsMetadata = loadedSkills.length > 0 ? loadedSkills : request.state?.skillsMetadata || [];
			const skillsLocations = formatSkillsLocations(sources);
			const skillsList = formatSkillsList(skillsMetadata, sources);
			const skillsSection = SKILLS_SYSTEM_PROMPT.replace("{skills_locations}", skillsLocations).replace("{skills_list}", skillsList);
			const newSystemMessage = request.systemMessage.concat(skillsSection);
			return handler({
				...request,
				systemMessage: newSystemMessage
			});
		}
	});
}

//#endregion
//#region src/middleware/summarization.ts
/**
* Summarization middleware with backend support for conversation history offloading.
*
* This module extends the base LangChain summarization middleware with additional
* backend-based features for persisting conversation history before summarization.
*
* ## Usage
*
* ```typescript
* import { createSummarizationMiddleware } from "@anthropic/deepagents";
* import { FilesystemBackend } from "@anthropic/deepagents";
*
* const backend = new FilesystemBackend({ rootDir: "/data" });
*
* const middleware = createSummarizationMiddleware({
*   model: "gpt-4o-mini",
*   backend,
*   trigger: { type: "fraction", value: 0.85 },
*   keep: { type: "fraction", value: 0.10 },
* });
*
* const agent = createDeepAgent({ middleware: [middleware] });
* ```
*
* ## Storage
*
* Offloaded messages are stored as markdown at `/conversation_history/{thread_id}.md`.
*
* Each summarization event appends a new section to this file, creating a running log
* of all evicted messages.
*
* ## Relationship to LangChain Summarization Middleware
*
* The base `summarizationMiddleware` from `langchain` provides core summarization
* functionality. This middleware adds:
* - Backend-based conversation history offloading
* - Tool argument truncation for old messages
*
* For simple use cases without backend offloading, use `summarizationMiddleware`
* from `langchain` directly.
*/
/**
* Zod schema for a summarization event that tracks what was summarized and
* where the cutoff is.
*
* Instead of rewriting LangGraph state with `RemoveMessage(REMOVE_ALL_MESSAGES)`,
* the middleware stores this event and uses it to reconstruct the effective message
* list on subsequent calls.
*/
const SummarizationEventSchema = z$1.object({
	cutoffIndex: z$1.number(),
	summaryMessage: z$1.instanceof(HumanMessage),
	filePath: z$1.string().nullable()
});
/**
* State schema for summarization middleware.
*/
const SummarizationStateSchema = z$1.object({
	_summarizationSessionId: z$1.string().optional(),
	_summarizationEvent: SummarizationEventSchema.optional()
});

//#endregion
//#region src/backends/store.ts
/**
* Backend that stores files in LangGraph's BaseStore (persistent).
*
* Uses LangGraph's Store for persistent, cross-conversation storage.
* Files are organized via namespaces and persist across all threads.
*
* The namespace can include an optional assistant_id for multi-agent isolation.
*/
var StoreBackend = class {
	stateAndStore;
	constructor(stateAndStore) {
		this.stateAndStore = stateAndStore;
	}
	/**
	* Get the store instance.
	*
	* @returns BaseStore instance
	* @throws Error if no store is available
	*/
	getStore() {
		const store = this.stateAndStore.store;
		if (!store) throw new Error("Store is required but not available in StateAndStore");
		return store;
	}
	/**
	* Get the namespace for store operations.
	*
	* If an assistant_id is available in stateAndStore, return
	* [assistant_id, "filesystem"] to provide per-assistant isolation.
	* Otherwise return ["filesystem"].
	*/
	getNamespace() {
		const namespace = "filesystem";
		const assistantId = this.stateAndStore.assistantId;
		if (assistantId) return [assistantId, namespace];
		return [namespace];
	}
	/**
	* Convert a store Item to FileData format.
	*
	* @param storeItem - The store Item containing file data
	* @returns FileData object
	* @throws Error if required fields are missing or have incorrect types
	*/
	convertStoreItemToFileData(storeItem) {
		const value = storeItem.value;
		if (!value.content || !Array.isArray(value.content) || typeof value.created_at !== "string" || typeof value.modified_at !== "string") throw new Error(`Store item does not contain valid FileData fields. Got keys: ${Object.keys(value).join(", ")}`);
		return {
			content: value.content,
			created_at: value.created_at,
			modified_at: value.modified_at
		};
	}
	/**
	* Convert FileData to a value suitable for store.put().
	*
	* @param fileData - The FileData to convert
	* @returns Object with content, created_at, and modified_at fields
	*/
	convertFileDataToStoreValue(fileData) {
		return {
			content: fileData.content,
			created_at: fileData.created_at,
			modified_at: fileData.modified_at
		};
	}
	/**
	* Search store with automatic pagination to retrieve all results.
	*
	* @param store - The store to search
	* @param namespace - Hierarchical path prefix to search within
	* @param options - Optional query, filter, and page_size
	* @returns List of all items matching the search criteria
	*/
	async searchStorePaginated(store, namespace, options = {}) {
		const { query, filter, pageSize = 100 } = options;
		const allItems = [];
		let offset = 0;
		while (true) {
			const pageItems = await store.search(namespace, {
				query,
				filter,
				limit: pageSize,
				offset
			});
			if (!pageItems || pageItems.length === 0) break;
			allItems.push(...pageItems);
			if (pageItems.length < pageSize) break;
			offset += pageSize;
		}
		return allItems;
	}
	/**
	* List files and directories in the specified directory (non-recursive).
	*
	* @param path - Absolute path to directory
	* @returns List of FileInfo objects for files and directories directly in the directory.
	*          Directories have a trailing / in their path and is_dir=true.
	*/
	async lsInfo(path) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const items = await this.searchStorePaginated(store, namespace);
		const infos = [];
		const subdirs = /* @__PURE__ */ new Set();
		const normalizedPath = path.endsWith("/") ? path : path + "/";
		for (const item of items) {
			const itemKey = String(item.key);
			if (!itemKey.startsWith(normalizedPath)) continue;
			const relative = itemKey.substring(normalizedPath.length);
			if (relative.includes("/")) {
				const subdirName = relative.split("/")[0];
				subdirs.add(normalizedPath + subdirName + "/");
				continue;
			}
			try {
				const fd = this.convertStoreItemToFileData(item);
				const size = fd.content.join("\n").length;
				infos.push({
					path: itemKey,
					is_dir: false,
					size,
					modified_at: fd.modified_at
				});
			} catch {
				continue;
			}
		}
		for (const subdir of Array.from(subdirs).sort()) infos.push({
			path: subdir,
			is_dir: true,
			size: 0,
			modified_at: ""
		});
		infos.sort((a, b) => a.path.localeCompare(b.path));
		return infos;
	}
	/**
	* Read file content with line numbers.
	*
	* @param filePath - Absolute file path
	* @param offset - Line offset to start reading from (0-indexed)
	* @param limit - Maximum number of lines to read
	* @returns Formatted file content with line numbers, or error message
	*/
	async read(filePath, offset = 0, limit = 500) {
		try {
			return formatReadResponse(await this.readRaw(filePath), offset, limit);
		} catch (e) {
			return `Error: ${e.message}`;
		}
	}
	/**
	* Read file content as raw FileData.
	*
	* @param filePath - Absolute file path
	* @returns Raw file content as FileData
	*/
	async readRaw(filePath) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const item = await store.get(namespace, filePath);
		if (!item) throw new Error(`File '${filePath}' not found`);
		return this.convertStoreItemToFileData(item);
	}
	/**
	* Create a new file with content.
	* Returns WriteResult. External storage sets filesUpdate=null.
	*/
	async write(filePath, content) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		if (await store.get(namespace, filePath)) return { error: `Cannot write to ${filePath} because it already exists. Read and then make an edit, or write to a new path.` };
		const fileData = createFileData(content);
		const storeValue = this.convertFileDataToStoreValue(fileData);
		await store.put(namespace, filePath, storeValue);
		return {
			path: filePath,
			filesUpdate: null
		};
	}
	/**
	* Edit a file by replacing string occurrences.
	* Returns EditResult. External storage sets filesUpdate=null.
	*/
	async edit(filePath, oldString, newString, replaceAll = false) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const item = await store.get(namespace, filePath);
		if (!item) return { error: `Error: File '${filePath}' not found` };
		try {
			const fileData = this.convertStoreItemToFileData(item);
			const result = performStringReplacement(fileDataToString(fileData), oldString, newString, replaceAll);
			if (typeof result === "string") return { error: result };
			const [newContent, occurrences] = result;
			const newFileData = updateFileData(fileData, newContent);
			const storeValue = this.convertFileDataToStoreValue(newFileData);
			await store.put(namespace, filePath, storeValue);
			return {
				path: filePath,
				filesUpdate: null,
				occurrences
			};
		} catch (e) {
			return { error: `Error: ${e.message}` };
		}
	}
	/**
	* Structured search results or error string for invalid input.
	*/
	async grepRaw(pattern, path = "/", glob = null) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const items = await this.searchStorePaginated(store, namespace);
		const files = {};
		for (const item of items) try {
			files[item.key] = this.convertStoreItemToFileData(item);
		} catch {
			continue;
		}
		return grepMatchesFromFiles(files, pattern, path, glob);
	}
	/**
	* Structured glob matching returning FileInfo objects.
	*/
	async globInfo(pattern, path = "/") {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const items = await this.searchStorePaginated(store, namespace);
		const files = {};
		for (const item of items) try {
			files[item.key] = this.convertStoreItemToFileData(item);
		} catch {
			continue;
		}
		const result = globSearchFiles(files, pattern, path);
		if (result === "No files found") return [];
		const paths = result.split("\n");
		const infos = [];
		for (const p of paths) {
			const fd = files[p];
			const size = fd ? fd.content.join("\n").length : 0;
			infos.push({
				path: p,
				is_dir: false,
				size,
				modified_at: fd?.modified_at || ""
			});
		}
		return infos;
	}
	/**
	* Upload multiple files.
	*
	* @param files - List of [path, content] tuples to upload
	* @returns List of FileUploadResponse objects, one per input file
	*/
	async uploadFiles(files) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const responses = [];
		for (const [path, content] of files) try {
			const fileData = createFileData(new TextDecoder().decode(content));
			const storeValue = this.convertFileDataToStoreValue(fileData);
			await store.put(namespace, path, storeValue);
			responses.push({
				path,
				error: null
			});
		} catch {
			responses.push({
				path,
				error: "invalid_path"
			});
		}
		return responses;
	}
	/**
	* Download multiple files.
	*
	* @param paths - List of file paths to download
	* @returns List of FileDownloadResponse objects, one per input path
	*/
	async downloadFiles(paths) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const responses = [];
		for (const path of paths) try {
			const item = await store.get(namespace, path);
			if (!item) {
				responses.push({
					path,
					content: null,
					error: "file_not_found"
				});
				continue;
			}
			const contentStr = fileDataToString(this.convertStoreItemToFileData(item));
			const content = new TextEncoder().encode(contentStr);
			responses.push({
				path,
				content,
				error: null
			});
		} catch {
			responses.push({
				path,
				content: null,
				error: "file_not_found"
			});
		}
		return responses;
	}
};

//#endregion
//#region src/backends/filesystem.ts
/**
* FilesystemBackend: Read and write files directly from the filesystem.
*
* Security and search upgrades:
* - Secure path resolution with root containment when in virtual_mode (sandboxed to cwd)
* - Prevent symlink-following on file I/O using O_NOFOLLOW when available
* - Ripgrep-powered grep with literal (fixed-string) search, plus substring fallback
*   and optional glob include filtering, while preserving virtual path behavior
*/
const SUPPORTS_NOFOLLOW = fs$1.constants.O_NOFOLLOW !== void 0;
/**
* Backend that reads and writes files directly from the filesystem.
*
* Files are accessed using their actual filesystem paths. Relative paths are
* resolved relative to the current working directory. Content is read/written
* as plain text, and metadata (timestamps) are derived from filesystem stats.
*/
var FilesystemBackend = class {
	cwd;
	virtualMode;
	maxFileSizeBytes;
	constructor(options = {}) {
		const { rootDir, virtualMode = false, maxFileSizeMb = 10 } = options;
		this.cwd = rootDir ? path.resolve(rootDir) : process.cwd();
		this.virtualMode = virtualMode;
		this.maxFileSizeBytes = maxFileSizeMb * 1024 * 1024;
	}
	/**
	* Resolve a file path with security checks.
	*
	* When virtualMode=true, treat incoming paths as virtual absolute paths under
	* this.cwd, disallow traversal (.., ~) and ensure resolved path stays within root.
	* When virtualMode=false, preserve legacy behavior: absolute paths are allowed
	* as-is; relative paths resolve under cwd.
	*
	* @param key - File path (absolute, relative, or virtual when virtualMode=true)
	* @returns Resolved absolute path string
	* @throws Error if path traversal detected or path outside root
	*/
	resolvePath(key) {
		if (this.virtualMode) {
			const vpath = key.startsWith("/") ? key : "/" + key;
			if (vpath.includes("..") || vpath.startsWith("~")) throw new Error("Path traversal not allowed");
			const full = path.resolve(this.cwd, vpath.substring(1));
			const relative = path.relative(this.cwd, full);
			if (relative.startsWith("..") || path.isAbsolute(relative)) throw new Error(`Path: ${full} outside root directory: ${this.cwd}`);
			return full;
		}
		if (path.isAbsolute(key)) return key;
		return path.resolve(this.cwd, key);
	}
	/**
	* List files and directories in the specified directory (non-recursive).
	*
	* @param dirPath - Absolute directory path to list files from
	* @returns List of FileInfo objects for files and directories directly in the directory.
	*          Directories have a trailing / in their path and is_dir=true.
	*/
	async lsInfo(dirPath) {
		try {
			const resolvedPath = this.resolvePath(dirPath);
			if (!(await fs.stat(resolvedPath)).isDirectory()) return [];
			const entries = await fs.readdir(resolvedPath, { withFileTypes: true });
			const results = [];
			const cwdStr = this.cwd.endsWith(path.sep) ? this.cwd : this.cwd + path.sep;
			for (const entry of entries) {
				const fullPath = path.join(resolvedPath, entry.name);
				try {
					const entryStat = await fs.stat(fullPath);
					const isFile = entryStat.isFile();
					const isDir = entryStat.isDirectory();
					if (!this.virtualMode) {
						if (isFile) results.push({
							path: fullPath,
							is_dir: false,
							size: entryStat.size,
							modified_at: entryStat.mtime.toISOString()
						});
						else if (isDir) results.push({
							path: fullPath + path.sep,
							is_dir: true,
							size: 0,
							modified_at: entryStat.mtime.toISOString()
						});
					} else {
						let relativePath;
						if (fullPath.startsWith(cwdStr)) relativePath = fullPath.substring(cwdStr.length);
						else if (fullPath.startsWith(this.cwd)) relativePath = fullPath.substring(this.cwd.length).replace(/^[/\\]/, "");
						else relativePath = fullPath;
						relativePath = relativePath.split(path.sep).join("/");
						const virtPath = "/" + relativePath;
						if (isFile) results.push({
							path: virtPath,
							is_dir: false,
							size: entryStat.size,
							modified_at: entryStat.mtime.toISOString()
						});
						else if (isDir) results.push({
							path: virtPath + "/",
							is_dir: true,
							size: 0,
							modified_at: entryStat.mtime.toISOString()
						});
					}
				} catch {
					continue;
				}
			}
			results.sort((a, b) => a.path.localeCompare(b.path));
			return results;
		} catch {
			return [];
		}
	}
	/**
	* Read file content with line numbers.
	*
	* @param filePath - Absolute or relative file path
	* @param offset - Line offset to start reading from (0-indexed)
	* @param limit - Maximum number of lines to read
	* @returns Formatted file content with line numbers, or error message
	*/
	async read(filePath, offset = 0, limit = 500) {
		try {
			const resolvedPath = this.resolvePath(filePath);
			let content;
			if (SUPPORTS_NOFOLLOW) {
				if (!(await fs.stat(resolvedPath)).isFile()) return `Error: File '${filePath}' not found`;
				const fd = await fs.open(resolvedPath, fs$1.constants.O_RDONLY | fs$1.constants.O_NOFOLLOW);
				try {
					content = await fd.readFile({ encoding: "utf-8" });
				} finally {
					await fd.close();
				}
			} else {
				const stat = await fs.lstat(resolvedPath);
				if (stat.isSymbolicLink()) return `Error: Symlinks are not allowed: ${filePath}`;
				if (!stat.isFile()) return `Error: File '${filePath}' not found`;
				content = await fs.readFile(resolvedPath, "utf-8");
			}
			const emptyMsg = checkEmptyContent(content);
			if (emptyMsg) return emptyMsg;
			const lines = content.split("\n");
			const startIdx = offset;
			const endIdx = Math.min(startIdx + limit, lines.length);
			if (startIdx >= lines.length) return `Error: Line offset ${offset} exceeds file length (${lines.length} lines)`;
			return formatContentWithLineNumbers(lines.slice(startIdx, endIdx), startIdx + 1);
		} catch (e) {
			return `Error reading file '${filePath}': ${e.message}`;
		}
	}
	/**
	* Read file content as raw FileData.
	*
	* @param filePath - Absolute file path
	* @returns Raw file content as FileData
	*/
	async readRaw(filePath) {
		const resolvedPath = this.resolvePath(filePath);
		let content;
		let stat;
		if (SUPPORTS_NOFOLLOW) {
			stat = await fs.stat(resolvedPath);
			if (!stat.isFile()) throw new Error(`File '${filePath}' not found`);
			const fd = await fs.open(resolvedPath, fs$1.constants.O_RDONLY | fs$1.constants.O_NOFOLLOW);
			try {
				content = await fd.readFile({ encoding: "utf-8" });
			} finally {
				await fd.close();
			}
		} else {
			stat = await fs.lstat(resolvedPath);
			if (stat.isSymbolicLink()) throw new Error(`Symlinks are not allowed: ${filePath}`);
			if (!stat.isFile()) throw new Error(`File '${filePath}' not found`);
			content = await fs.readFile(resolvedPath, "utf-8");
		}
		return {
			content: content.split("\n"),
			created_at: stat.ctime.toISOString(),
			modified_at: stat.mtime.toISOString()
		};
	}
	/**
	* Create a new file with content.
	* Returns WriteResult. External storage sets filesUpdate=null.
	*/
	async write(filePath, content) {
		try {
			const resolvedPath = this.resolvePath(filePath);
			try {
				if ((await fs.lstat(resolvedPath)).isSymbolicLink()) return { error: `Cannot write to ${filePath} because it is a symlink. Symlinks are not allowed.` };
				return { error: `Cannot write to ${filePath} because it already exists. Read and then make an edit, or write to a new path.` };
			} catch {}
			await fs.mkdir(path.dirname(resolvedPath), { recursive: true });
			if (SUPPORTS_NOFOLLOW) {
				const flags = fs$1.constants.O_WRONLY | fs$1.constants.O_CREAT | fs$1.constants.O_TRUNC | fs$1.constants.O_NOFOLLOW;
				const fd = await fs.open(resolvedPath, flags, 420);
				try {
					await fd.writeFile(content, "utf-8");
				} finally {
					await fd.close();
				}
			} else await fs.writeFile(resolvedPath, content, "utf-8");
			return {
				path: filePath,
				filesUpdate: null
			};
		} catch (e) {
			return { error: `Error writing file '${filePath}': ${e.message}` };
		}
	}
	/**
	* Edit a file by replacing string occurrences.
	* Returns EditResult. External storage sets filesUpdate=null.
	*/
	async edit(filePath, oldString, newString, replaceAll = false) {
		try {
			const resolvedPath = this.resolvePath(filePath);
			let content;
			if (SUPPORTS_NOFOLLOW) {
				if (!(await fs.stat(resolvedPath)).isFile()) return { error: `Error: File '${filePath}' not found` };
				const fd = await fs.open(resolvedPath, fs$1.constants.O_RDONLY | fs$1.constants.O_NOFOLLOW);
				try {
					content = await fd.readFile({ encoding: "utf-8" });
				} finally {
					await fd.close();
				}
			} else {
				const stat = await fs.lstat(resolvedPath);
				if (stat.isSymbolicLink()) return { error: `Error: Symlinks are not allowed: ${filePath}` };
				if (!stat.isFile()) return { error: `Error: File '${filePath}' not found` };
				content = await fs.readFile(resolvedPath, "utf-8");
			}
			const result = performStringReplacement(content, oldString, newString, replaceAll);
			if (typeof result === "string") return { error: result };
			const [newContent, occurrences] = result;
			if (SUPPORTS_NOFOLLOW) {
				const flags = fs$1.constants.O_WRONLY | fs$1.constants.O_TRUNC | fs$1.constants.O_NOFOLLOW;
				const fd = await fs.open(resolvedPath, flags);
				try {
					await fd.writeFile(newContent, "utf-8");
				} finally {
					await fd.close();
				}
			} else await fs.writeFile(resolvedPath, newContent, "utf-8");
			return {
				path: filePath,
				filesUpdate: null,
				occurrences
			};
		} catch (e) {
			return { error: `Error editing file '${filePath}': ${e.message}` };
		}
	}
	/**
	* Search for a literal text pattern in files.
	*
	* Uses ripgrep if available, falling back to substring search.
	*
	* @param pattern - Literal string to search for (NOT regex).
	* @param dirPath - Directory or file path to search in. Defaults to current directory.
	* @param glob - Optional glob pattern to filter which files to search.
	* @returns List of GrepMatch dicts containing path, line number, and matched text.
	*/
	async grepRaw(pattern, dirPath = "/", glob = null) {
		let baseFull;
		try {
			baseFull = this.resolvePath(dirPath || ".");
		} catch {
			return [];
		}
		try {
			await fs.stat(baseFull);
		} catch {
			return [];
		}
		let results = await this.ripgrepSearch(pattern, baseFull, glob);
		if (results === null) results = await this.literalSearch(pattern, baseFull, glob);
		const matches = [];
		for (const [fpath, items] of Object.entries(results)) for (const [lineNum, lineText] of items) matches.push({
			path: fpath,
			line: lineNum,
			text: lineText
		});
		return matches;
	}
	/**
	* Search using ripgrep with fixed-string (literal) mode.
	*
	* @param pattern - Literal string to search for (unescaped).
	* @param baseFull - Resolved base path to search in.
	* @param includeGlob - Optional glob pattern to filter files.
	* @returns Dict mapping file paths to list of (line_number, line_text) tuples.
	*          Returns null if ripgrep is unavailable or times out.
	*/
	async ripgrepSearch(pattern, baseFull, includeGlob) {
		return new Promise((resolve) => {
			const args = ["--json", "-F"];
			if (includeGlob) args.push("--glob", includeGlob);
			args.push("--", pattern, baseFull);
			const proc = spawn("rg", args, { timeout: 3e4 });
			const results = {};
			let output = "";
			proc.stdout.on("data", (data) => {
				output += data.toString();
			});
			proc.on("close", (code) => {
				if (code !== 0 && code !== 1) {
					resolve(null);
					return;
				}
				for (const line of output.split("\n")) {
					if (!line.trim()) continue;
					try {
						const data = JSON.parse(line);
						if (data.type !== "match") continue;
						const pdata = data.data || {};
						const ftext = pdata.path?.text;
						if (!ftext) continue;
						let virtPath;
						if (this.virtualMode) try {
							const resolved = path.resolve(ftext);
							const relative = path.relative(this.cwd, resolved);
							if (relative.startsWith("..")) continue;
							virtPath = "/" + relative.split(path.sep).join("/");
						} catch {
							continue;
						}
						else virtPath = ftext;
						const ln = pdata.line_number;
						const lt = pdata.lines?.text?.replace(/\n$/, "") || "";
						if (ln === void 0) continue;
						if (!results[virtPath]) results[virtPath] = [];
						results[virtPath].push([ln, lt]);
					} catch {
						continue;
					}
				}
				resolve(results);
			});
			proc.on("error", () => {
				resolve(null);
			});
		});
	}
	/**
	* Fallback search using literal substring matching when ripgrep is unavailable.
	*
	* Recursively searches files, respecting maxFileSizeBytes limit.
	*
	* @param pattern - Literal string to search for.
	* @param baseFull - Resolved base path to search in.
	* @param includeGlob - Optional glob pattern to filter files by name.
	* @returns Dict mapping file paths to list of (line_number, line_text) tuples.
	*/
	async literalSearch(pattern, baseFull, includeGlob) {
		const results = {};
		const files = await fg("**/*", {
			cwd: (await fs.stat(baseFull)).isDirectory() ? baseFull : path.dirname(baseFull),
			absolute: true,
			onlyFiles: true,
			dot: true
		});
		for (const fp of files) try {
			if (includeGlob && !micromatch.isMatch(path.basename(fp), includeGlob)) continue;
			if ((await fs.stat(fp)).size > this.maxFileSizeBytes) continue;
			const lines = (await fs.readFile(fp, "utf-8")).split("\n");
			for (let i = 0; i < lines.length; i++) {
				const line = lines[i];
				if (line.includes(pattern)) {
					let virtPath;
					if (this.virtualMode) try {
						const relative = path.relative(this.cwd, fp);
						if (relative.startsWith("..")) continue;
						virtPath = "/" + relative.split(path.sep).join("/");
					} catch {
						continue;
					}
					else virtPath = fp;
					if (!results[virtPath]) results[virtPath] = [];
					results[virtPath].push([i + 1, line]);
				}
			}
		} catch {
			continue;
		}
		return results;
	}
	/**
	* Structured glob matching returning FileInfo objects.
	*/
	async globInfo(pattern, searchPath = "/") {
		if (pattern.startsWith("/")) pattern = pattern.substring(1);
		const resolvedSearchPath = searchPath === "/" ? this.cwd : this.resolvePath(searchPath);
		try {
			if (!(await fs.stat(resolvedSearchPath)).isDirectory()) return [];
		} catch {
			return [];
		}
		const results = [];
		try {
			const matches = await fg(pattern, {
				cwd: resolvedSearchPath,
				absolute: true,
				onlyFiles: true,
				dot: true
			});
			for (const matchedPath of matches) try {
				const stat = await fs.stat(matchedPath);
				if (!stat.isFile()) continue;
				const normalizedPath = matchedPath.split("/").join(path.sep);
				if (!this.virtualMode) results.push({
					path: normalizedPath,
					is_dir: false,
					size: stat.size,
					modified_at: stat.mtime.toISOString()
				});
				else {
					const cwdStr = this.cwd.endsWith(path.sep) ? this.cwd : this.cwd + path.sep;
					let relativePath;
					if (normalizedPath.startsWith(cwdStr)) relativePath = normalizedPath.substring(cwdStr.length);
					else if (normalizedPath.startsWith(this.cwd)) relativePath = normalizedPath.substring(this.cwd.length).replace(/^[/\\]/, "");
					else relativePath = normalizedPath;
					relativePath = relativePath.split(path.sep).join("/");
					const virt = "/" + relativePath;
					results.push({
						path: virt,
						is_dir: false,
						size: stat.size,
						modified_at: stat.mtime.toISOString()
					});
				}
			} catch {
				continue;
			}
		} catch {}
		results.sort((a, b) => a.path.localeCompare(b.path));
		return results;
	}
	/**
	* Upload multiple files to the filesystem.
	*
	* @param files - List of [path, content] tuples to upload
	* @returns List of FileUploadResponse objects, one per input file
	*/
	async uploadFiles(files) {
		const responses = [];
		for (const [filePath, content] of files) try {
			const resolvedPath = this.resolvePath(filePath);
			await fs.mkdir(path.dirname(resolvedPath), { recursive: true });
			await fs.writeFile(resolvedPath, content);
			responses.push({
				path: filePath,
				error: null
			});
		} catch (e) {
			if (e.code === "ENOENT") responses.push({
				path: filePath,
				error: "file_not_found"
			});
			else if (e.code === "EACCES") responses.push({
				path: filePath,
				error: "permission_denied"
			});
			else if (e.code === "EISDIR") responses.push({
				path: filePath,
				error: "is_directory"
			});
			else responses.push({
				path: filePath,
				error: "invalid_path"
			});
		}
		return responses;
	}
	/**
	* Download multiple files from the filesystem.
	*
	* @param paths - List of file paths to download
	* @returns List of FileDownloadResponse objects, one per input path
	*/
	async downloadFiles(paths) {
		const responses = [];
		for (const filePath of paths) try {
			const resolvedPath = this.resolvePath(filePath);
			const content = await fs.readFile(resolvedPath);
			responses.push({
				path: filePath,
				content,
				error: null
			});
		} catch (e) {
			if (e.code === "ENOENT") responses.push({
				path: filePath,
				content: null,
				error: "file_not_found"
			});
			else if (e.code === "EACCES") responses.push({
				path: filePath,
				content: null,
				error: "permission_denied"
			});
			else if (e.code === "EISDIR") responses.push({
				path: filePath,
				content: null,
				error: "is_directory"
			});
			else responses.push({
				path: filePath,
				content: null,
				error: "invalid_path"
			});
		}
		return responses;
	}
};

//#endregion
//#region src/backends/composite.ts
/**
* Backend that routes file operations to different backends based on path prefix.
*
* This enables hybrid storage strategies like:
* - `/memories/`  StoreBackend (persistent, cross-thread)
* - Everything else  StateBackend (ephemeral, per-thread)
*
* The CompositeBackend handles path prefix stripping/re-adding transparently.
*/
var CompositeBackend = class {
	default;
	routes;
	sortedRoutes;
	constructor(defaultBackend, routes) {
		this.default = defaultBackend;
		this.routes = routes;
		this.sortedRoutes = Object.entries(routes).sort((a, b) => b[0].length - a[0].length);
	}
	/**
	* Determine which backend handles this key and strip prefix.
	*
	* @param key - Original file path
	* @returns Tuple of [backend, stripped_key] where stripped_key has the route
	*          prefix removed (but keeps leading slash).
	*/
	getBackendAndKey(key) {
		for (const [prefix, backend] of this.sortedRoutes) if (key.startsWith(prefix)) {
			const suffix = key.substring(prefix.length);
			return [backend, suffix ? "/" + suffix : "/"];
		}
		return [this.default, key];
	}
	/**
	* List files and directories in the specified directory (non-recursive).
	*
	* @param path - Absolute path to directory
	* @returns List of FileInfo objects with route prefixes added, for files and directories
	*          directly in the directory. Directories have a trailing / in their path and is_dir=true.
	*/
	async lsInfo(path) {
		for (const [routePrefix, backend] of this.sortedRoutes) if (path.startsWith(routePrefix.replace(/\/$/, ""))) {
			const suffix = path.substring(routePrefix.length);
			const searchPath = suffix ? "/" + suffix : "/";
			const infos = await backend.lsInfo(searchPath);
			const prefixed = [];
			for (const fi of infos) prefixed.push({
				...fi,
				path: routePrefix.slice(0, -1) + fi.path
			});
			return prefixed;
		}
		if (path === "/") {
			const results = [];
			const defaultInfos = await this.default.lsInfo(path);
			results.push(...defaultInfos);
			for (const [routePrefix] of this.sortedRoutes) results.push({
				path: routePrefix,
				is_dir: true,
				size: 0,
				modified_at: ""
			});
			results.sort((a, b) => a.path.localeCompare(b.path));
			return results;
		}
		return await this.default.lsInfo(path);
	}
	/**
	* Read file content, routing to appropriate backend.
	*
	* @param filePath - Absolute file path
	* @param offset - Line offset to start reading from (0-indexed)
	* @param limit - Maximum number of lines to read
	* @returns Formatted file content with line numbers, or error message
	*/
	async read(filePath, offset = 0, limit = 500) {
		const [backend, strippedKey] = this.getBackendAndKey(filePath);
		return await backend.read(strippedKey, offset, limit);
	}
	/**
	* Read file content as raw FileData.
	*
	* @param filePath - Absolute file path
	* @returns Raw file content as FileData
	*/
	async readRaw(filePath) {
		const [backend, strippedKey] = this.getBackendAndKey(filePath);
		return await backend.readRaw(strippedKey);
	}
	/**
	* Structured search results or error string for invalid input.
	*/
	async grepRaw(pattern, path = "/", glob = null) {
		for (const [routePrefix, backend] of this.sortedRoutes) if (path.startsWith(routePrefix.replace(/\/$/, ""))) {
			const searchPath = path.substring(routePrefix.length - 1);
			const raw = await backend.grepRaw(pattern, searchPath || "/", glob);
			if (typeof raw === "string") return raw;
			return raw.map((m) => ({
				...m,
				path: routePrefix.slice(0, -1) + m.path
			}));
		}
		const allMatches = [];
		const rawDefault = await this.default.grepRaw(pattern, path, glob);
		if (typeof rawDefault === "string") return rawDefault;
		allMatches.push(...rawDefault);
		for (const [routePrefix, backend] of Object.entries(this.routes)) {
			const raw = await backend.grepRaw(pattern, "/", glob);
			if (typeof raw === "string") return raw;
			allMatches.push(...raw.map((m) => ({
				...m,
				path: routePrefix.slice(0, -1) + m.path
			})));
		}
		return allMatches;
	}
	/**
	* Structured glob matching returning FileInfo objects.
	*/
	async globInfo(pattern, path = "/") {
		const results = [];
		for (const [routePrefix, backend] of this.sortedRoutes) if (path.startsWith(routePrefix.replace(/\/$/, ""))) {
			const searchPath = path.substring(routePrefix.length - 1);
			return (await backend.globInfo(pattern, searchPath || "/")).map((fi) => ({
				...fi,
				path: routePrefix.slice(0, -1) + fi.path
			}));
		}
		const defaultInfos = await this.default.globInfo(pattern, path);
		results.push(...defaultInfos);
		for (const [routePrefix, backend] of Object.entries(this.routes)) {
			const infos = await backend.globInfo(pattern, "/");
			results.push(...infos.map((fi) => ({
				...fi,
				path: routePrefix.slice(0, -1) + fi.path
			})));
		}
		results.sort((a, b) => a.path.localeCompare(b.path));
		return results;
	}
	/**
	* Create a new file, routing to appropriate backend.
	*
	* @param filePath - Absolute file path
	* @param content - File content as string
	* @returns WriteResult with path or error
	*/
	async write(filePath, content) {
		const [backend, strippedKey] = this.getBackendAndKey(filePath);
		return await backend.write(strippedKey, content);
	}
	/**
	* Edit a file, routing to appropriate backend.
	*
	* @param filePath - Absolute file path
	* @param oldString - String to find and replace
	* @param newString - Replacement string
	* @param replaceAll - If true, replace all occurrences
	* @returns EditResult with path, occurrences, or error
	*/
	async edit(filePath, oldString, newString, replaceAll = false) {
		const [backend, strippedKey] = this.getBackendAndKey(filePath);
		return await backend.edit(strippedKey, oldString, newString, replaceAll);
	}
	/**
	* Execute a command via the default backend.
	* Execution is not path-specific, so it always delegates to the default backend.
	*
	* @param command - Full shell command string to execute
	* @returns ExecuteResponse with combined output, exit code, and truncation flag
	* @throws Error if the default backend doesn't support command execution
	*/
	execute(command) {
		if (!isSandboxBackend(this.default)) throw new Error("Default backend doesn't support command execution (SandboxBackendProtocol). To enable execution, provide a default backend that implements SandboxBackendProtocol.");
		return Promise.resolve(this.default.execute(command));
	}
	/**
	* Upload multiple files, batching by backend for efficiency.
	*
	* @param files - List of [path, content] tuples to upload
	* @returns List of FileUploadResponse objects, one per input file
	*/
	async uploadFiles(files) {
		const results = Array.from({ length: files.length }, () => null);
		const batchesByBackend = /* @__PURE__ */ new Map();
		for (let idx = 0; idx < files.length; idx++) {
			const [path, content] = files[idx];
			const [backend, strippedPath] = this.getBackendAndKey(path);
			if (!batchesByBackend.has(backend)) batchesByBackend.set(backend, []);
			batchesByBackend.get(backend).push({
				idx,
				path: strippedPath,
				content
			});
		}
		for (const [backend, batch] of batchesByBackend) {
			if (!backend.uploadFiles) throw new Error("Backend does not support uploadFiles");
			const batchFiles = batch.map((b) => [b.path, b.content]);
			const batchResponses = await backend.uploadFiles(batchFiles);
			for (let i = 0; i < batch.length; i++) {
				const originalIdx = batch[i].idx;
				results[originalIdx] = {
					path: files[originalIdx][0],
					error: batchResponses[i]?.error ?? null
				};
			}
		}
		return results;
	}
	/**
	* Download multiple files, batching by backend for efficiency.
	*
	* @param paths - List of file paths to download
	* @returns List of FileDownloadResponse objects, one per input path
	*/
	async downloadFiles(paths) {
		const results = Array.from({ length: paths.length }, () => null);
		const batchesByBackend = /* @__PURE__ */ new Map();
		for (let idx = 0; idx < paths.length; idx++) {
			const path = paths[idx];
			const [backend, strippedPath] = this.getBackendAndKey(path);
			if (!batchesByBackend.has(backend)) batchesByBackend.set(backend, []);
			batchesByBackend.get(backend).push({
				idx,
				path: strippedPath
			});
		}
		for (const [backend, batch] of batchesByBackend) {
			if (!backend.downloadFiles) throw new Error("Backend does not support downloadFiles");
			const batchPaths = batch.map((b) => b.path);
			const batchResponses = await backend.downloadFiles(batchPaths);
			for (let i = 0; i < batch.length; i++) {
				const originalIdx = batch[i].idx;
				results[originalIdx] = {
					path: paths[originalIdx],
					content: batchResponses[i]?.content ?? null,
					error: batchResponses[i]?.error ?? null
				};
			}
		}
		return results;
	}
};

//#endregion
//#region src/backends/sandbox.ts
/**
* Shell-quote a string using single quotes (POSIX).
* Escapes embedded single quotes with the '\'' technique.
*/
function shellQuote(s) {
	return "'" + s.replace(/'/g, "'\\''") + "'";
}
/**
* Convert a glob pattern to a path-aware RegExp.
*
* Inspired by the just-bash project's glob utilities:
* - `*`  matches any characters except `/`
* - `**` matches any characters including `/` (recursive)
* - `?`  matches a single character except `/`
* - `[...]` character classes
*/
function globToPathRegex(pattern) {
	let regex = "^";
	let i = 0;
	while (i < pattern.length) {
		const c = pattern[i];
		if (c === "*") if (i + 1 < pattern.length && pattern[i + 1] === "*") {
			i += 2;
			if (i < pattern.length && pattern[i] === "/") {
				regex += "(.*/)?";
				i++;
			} else regex += ".*";
		} else {
			regex += "[^/]*";
			i++;
		}
		else if (c === "?") {
			regex += "[^/]";
			i++;
		} else if (c === "[") {
			let j = i + 1;
			while (j < pattern.length && pattern[j] !== "]") j++;
			regex += pattern.slice(i, j + 1);
			i = j + 1;
		} else if (c === "." || c === "+" || c === "^" || c === "$" || c === "{" || c === "}" || c === "(" || c === ")" || c === "|" || c === "\\") {
			regex += `\\${c}`;
			i++;
		} else {
			regex += c;
			i++;
		}
	}
	regex += "$";
	return new RegExp(regex);
}
/**
* Parse a single line of stat/find output in the format: size\tmtime\ttype\tpath
*
* The first three tab-delimited fields are always fixed (number, number, string),
* so we safely take everything after the third tab as the file path  even if the
* path itself contains tabs.
*
* The type field varies by platform / tool:
* - GNU find -printf %y: single letter "d", "f", "l"
* - BSD stat -f %Sp: permission strings like "drwxr-xr-x", "-rw-r--r--"
*
* The mtime field may be a float (GNU find %T@  "1234567890.0000000000")
* or an integer (BSD stat %m  "1234567890"); parseInt handles both.
*/
function parseStatLine(line) {
	const firstTab = line.indexOf("	");
	if (firstTab === -1) return null;
	const secondTab = line.indexOf("	", firstTab + 1);
	if (secondTab === -1) return null;
	const thirdTab = line.indexOf("	", secondTab + 1);
	if (thirdTab === -1) return null;
	const size = parseInt(line.slice(0, firstTab), 10);
	const mtime = parseInt(line.slice(firstTab + 1, secondTab), 10);
	const fileType = line.slice(secondTab + 1, thirdTab);
	const fullPath = line.slice(thirdTab + 1);
	if (isNaN(size) || isNaN(mtime)) return null;
	return {
		size,
		mtime,
		isDir: fileType === "d" || fileType === "directory" || fileType.startsWith("d"),
		fullPath
	};
}
/**
* BusyBox/Alpine fallback script for stat -c.
*
* Determines file type with POSIX test builtins, then uses stat -c
* (supported by both GNU coreutils and BusyBox) for size and mtime.
* printf handles tab-delimited output formatting.
*/
const STAT_C_SCRIPT = "for f; do if [ -d \"$f\" ]; then t=d; elif [ -L \"$f\" ]; then t=l; else t=f; fi; sz=$(stat -c %s \"$f\" 2>/dev/null) || continue; mt=$(stat -c %Y \"$f\" 2>/dev/null) || continue; printf \"%s\\t%s\\t%s\\t%s\\n\" \"$sz\" \"$mt\" \"$t\" \"$f\"; done";
/**
* Shell command for listing directory contents with metadata.
*
* Detects the environment at runtime with three-way probing:
* 1. GNU find (full Linux): uses built-in `-printf` (most efficient)
* 2. BusyBox / Alpine: uses `find -exec sh -c` with `stat -c` fallback
* 3. BSD / macOS: uses `find -exec stat -f`
*
* Output format per line: size\tmtime\ttype\tpath
*/
function buildLsCommand(dirPath) {
	const quotedPath = shellQuote(dirPath);
	const findBase = `find ${quotedPath} -maxdepth 1 -not -path ${quotedPath}`;
	return `if find /dev/null -maxdepth 0 -printf '' 2>/dev/null; then ${findBase} -printf '%s\\t%T@\\t%y\\t%p\\n' 2>/dev/null; elif stat -c %s /dev/null >/dev/null 2>&1; then ${findBase} -exec sh -c '${STAT_C_SCRIPT}' _ {} +; else ${findBase} -exec stat -f '%z\t%m\t%Sp\t%N' {} + 2>/dev/null; fi || true`;
}
/**
* Shell command for listing files recursively with metadata.
* Same three-way detection as buildLsCommand (GNU -printf / stat -c / BSD stat -f).
*
* Output format per line: size\tmtime\ttype\tpath
*/
function buildFindCommand(searchPath) {
	const quotedPath = shellQuote(searchPath);
	const findBase = `find ${quotedPath} -not -path ${quotedPath}`;
	return `if find /dev/null -maxdepth 0 -printf '' 2>/dev/null; then ${findBase} -printf '%s\\t%T@\\t%y\\t%p\\n' 2>/dev/null; elif stat -c %s /dev/null >/dev/null 2>&1; then ${findBase} -exec sh -c '${STAT_C_SCRIPT}' _ {} +; else ${findBase} -exec stat -f '%z\t%m\t%Sp\t%N' {} + 2>/dev/null; fi || true`;
}
/**
* Pure POSIX shell command for reading files with line numbers.
* Uses awk for line numbering with offset/limit  works on any Linux including Alpine.
*/
function buildReadCommand(filePath, offset, limit) {
	const quotedPath = shellQuote(filePath);
	const safeOffset = Number.isFinite(offset) && offset > 0 ? Math.floor(offset) : 0;
	const safeLimit = Number.isFinite(limit) && limit > 0 ? Math.min(Math.floor(limit), 999999999) : 999999999;
	const start = safeOffset + 1;
	const end = safeOffset + safeLimit;
	return [
		`if [ ! -f ${quotedPath} ]; then echo "Error: File not found"; exit 1; fi`,
		`if [ ! -s ${quotedPath} ]; then echo "System reminder: File exists but has empty contents"; exit 0; fi`,
		`awk 'NR >= ${start} && NR <= ${end} { printf "%6d\\t%s\\n", NR, $0 }' ${quotedPath}`
	].join("; ");
}
/**
* Build a grep command for literal (fixed-string) search.
* Uses grep -rHnF for recursive, with-filename, with-line-number, fixed-string search.
*
* When a glob pattern is provided, uses `find -name GLOB -exec grep` instead of
* `grep --include=GLOB` for universal compatibility (BusyBox grep lacks --include).
*
* @param pattern - Literal string to search for (NOT regex).
* @param searchPath - Base path to search in.
* @param globPattern - Optional glob pattern to filter files.
*/
function buildGrepCommand(pattern, searchPath, globPattern) {
	const patternEscaped = shellQuote(pattern);
	const searchPathQuoted = shellQuote(searchPath);
	if (globPattern) return `find ${searchPathQuoted} -type f -name ${shellQuote(globPattern)} -exec grep -HnF -e ${patternEscaped} {} + 2>/dev/null || true`;
	return `grep -rHnF -e ${patternEscaped} ${searchPathQuoted} 2>/dev/null || true`;
}
/**
* Base sandbox implementation with execute() as the only abstract method.
*
* This class provides default implementations for all SandboxBackendProtocol
* methods using shell commands executed via execute(). Concrete implementations
* only need to implement execute(), uploadFiles(), and downloadFiles().
*
* All shell commands use pure POSIX utilities (awk, grep, find, stat) that are
* available on any Linux including Alpine/busybox. No Python, Node.js, or
* other runtime is required on the sandbox host.
*/
var BaseSandbox = class {
	/**
	* List files and directories in the specified directory (non-recursive).
	*
	* Uses pure POSIX shell (find + stat) via execute()  works on any Linux
	* including Alpine. No Python or Node.js needed.
	*
	* @param path - Absolute path to directory
	* @returns List of FileInfo objects for files and directories directly in the directory.
	*/
	async lsInfo(path) {
		const command = buildLsCommand(path);
		const result = await this.execute(command);
		const infos = [];
		const lines = result.output.trim().split("\n").filter(Boolean);
		for (const line of lines) {
			const parsed = parseStatLine(line);
			if (!parsed) continue;
			infos.push({
				path: parsed.isDir ? parsed.fullPath + "/" : parsed.fullPath,
				is_dir: parsed.isDir,
				size: parsed.size,
				modified_at: (/* @__PURE__ */ new Date(parsed.mtime * 1e3)).toISOString()
			});
		}
		return infos;
	}
	/**
	* Read file content with line numbers.
	*
	* Uses pure POSIX shell (awk) via execute()  only the requested slice
	* is returned over the wire, making this efficient for large files.
	* Works on any Linux including Alpine (no Python or Node.js needed).
	*
	* @param filePath - Absolute file path
	* @param offset - Line offset to start reading from (0-indexed)
	* @param limit - Maximum number of lines to read
	* @returns Formatted file content with line numbers, or error message
	*/
	async read(filePath, offset = 0, limit = 500) {
		if (limit === 0) return "";
		const command = buildReadCommand(filePath, offset, limit);
		const result = await this.execute(command);
		if (result.exitCode !== 0) return `Error: File '${filePath}' not found`;
		return result.output;
	}
	/**
	* Read file content as raw FileData.
	*
	* Uses downloadFiles() directly  no runtime needed on the sandbox host.
	*
	* @param filePath - Absolute file path
	* @returns Raw file content as FileData
	*/
	async readRaw(filePath) {
		const results = await this.downloadFiles([filePath]);
		if (results[0].error || !results[0].content) throw new Error(`File '${filePath}' not found`);
		const lines = new TextDecoder().decode(results[0].content).split("\n");
		const now = (/* @__PURE__ */ new Date()).toISOString();
		return {
			content: lines,
			created_at: now,
			modified_at: now
		};
	}
	/**
	* Search for a literal text pattern in files using grep.
	*
	* @param pattern - Literal string to search for (NOT regex).
	* @param path - Directory or file path to search in.
	* @param glob - Optional glob pattern to filter which files to search.
	* @returns List of GrepMatch dicts containing path, line number, and matched text.
	*/
	async grepRaw(pattern, path = "/", glob = null) {
		const command = buildGrepCommand(pattern, path, glob);
		const output = (await this.execute(command)).output.trim();
		if (!output) return [];
		const matches = [];
		for (const line of output.split("\n")) {
			const parts = line.split(":");
			if (parts.length >= 3) {
				const lineNum = parseInt(parts[1], 10);
				if (!isNaN(lineNum)) matches.push({
					path: parts[0],
					line: lineNum,
					text: parts.slice(2).join(":")
				});
			}
		}
		return matches;
	}
	/**
	* Structured glob matching returning FileInfo objects.
	*
	* Uses pure POSIX shell (find + stat) via execute() to list all files,
	* then applies glob-to-regex matching in TypeScript. No Python or Node.js
	* needed on the sandbox host.
	*
	* Glob patterns are matched against paths relative to the search base:
	* - `*`  matches any characters except `/`
	* - `**` matches any characters including `/` (recursive)
	* - `?`  matches a single character except `/`
	* - `[...]` character classes
	*/
	async globInfo(pattern, path = "/") {
		const command = buildFindCommand(path);
		const result = await this.execute(command);
		const regex = globToPathRegex(pattern);
		const infos = [];
		const lines = result.output.trim().split("\n").filter(Boolean);
		const basePath = path.endsWith("/") ? path.slice(0, -1) : path;
		for (const line of lines) {
			const parsed = parseStatLine(line);
			if (!parsed) continue;
			const relPath = parsed.fullPath.startsWith(basePath + "/") ? parsed.fullPath.slice(basePath.length + 1) : parsed.fullPath;
			if (regex.test(relPath)) infos.push({
				path: relPath,
				is_dir: parsed.isDir,
				size: parsed.size,
				modified_at: (/* @__PURE__ */ new Date(parsed.mtime * 1e3)).toISOString()
			});
		}
		return infos;
	}
	/**
	* Create a new file with content.
	*
	* Uses downloadFiles() to check existence and uploadFiles() to write.
	* No runtime needed on the sandbox host.
	*/
	async write(filePath, content) {
		try {
			const existCheck = await this.downloadFiles([filePath]);
			if (existCheck[0].content !== null && existCheck[0].error === null) return { error: `Cannot write to ${filePath} because it already exists. Read and then make an edit, or write to a new path.` };
		} catch {}
		const encoder = new TextEncoder();
		const results = await this.uploadFiles([[filePath, encoder.encode(content)]]);
		if (results[0].error) return { error: `Failed to write to ${filePath}: ${results[0].error}` };
		return {
			path: filePath,
			filesUpdate: null
		};
	}
	/**
	* Edit a file by replacing string occurrences.
	*
	* Uses downloadFiles() to read, performs string replacement in TypeScript,
	* then uploadFiles() to write back. No runtime needed on the sandbox host.
	*/
	async edit(filePath, oldString, newString, replaceAll = false) {
		const results = await this.downloadFiles([filePath]);
		if (results[0].error || !results[0].content) return { error: `Error: File '${filePath}' not found` };
		const text = new TextDecoder().decode(results[0].content);
		const count = text.split(oldString).length - 1;
		if (count === 0) return { error: `String not found in file '${filePath}'` };
		if (count > 1 && !replaceAll) return { error: `Multiple occurrences found in '${filePath}'. Use replaceAll=true to replace all.` };
		const newText = replaceAll ? text.split(oldString).join(newString) : text.replace(oldString, newString);
		const encoder = new TextEncoder();
		const uploadResults = await this.uploadFiles([[filePath, encoder.encode(newText)]]);
		if (uploadResults[0].error) return { error: `Failed to write edited file '${filePath}': ${uploadResults[0].error}` };
		return {
			path: filePath,
			filesUpdate: null,
			occurrences: count
		};
	}
};

//#endregion
//#region src/agent.ts
const BASE_PROMPT = `In order to complete the objective that the user asks of you, you have access to a number of standard tools.`;
/**
* Create a Deep Agent with middleware-based architecture.
*
* Matches Python's create_deep_agent function, using middleware for all features:
* - Todo management (todoListMiddleware)
* - Filesystem tools (createFilesystemMiddleware)
* - Subagent delegation (createSubAgentMiddleware)
* - Conversation summarization (summarizationMiddleware)
* - Prompt caching (anthropicPromptCachingMiddleware)
* - Tool call patching (createPatchToolCallsMiddleware)
* - Human-in-the-loop (humanInTheLoopMiddleware) - optional
*
* @param params Configuration parameters for the agent
* @returns ReactAgent instance ready for invocation with properly inferred state types
*
* @example
* ```typescript
* // Middleware with custom state
* const ResearchMiddleware = createMiddleware({
*   name: "ResearchMiddleware",
*   stateSchema: z.object({ research: z.string().default("") }),
* });
*
* const agent = createDeepAgent({
*   middleware: [ResearchMiddleware],
* });
*
* const result = await agent.invoke({ messages: [...] });
* // result.research is properly typed as string
* ```
*/
function createDeepAgent(params = {}) {
	const { model = "claude-sonnet-4-5-20250929", tools = [], systemPrompt, middleware: customMiddleware = [], subagents = [], responseFormat, contextSchema, checkpointer, store, backend, interruptOn, name, memory, skills } = params;
	/**
	* Combine system prompt with base prompt like Python implementation
	*/
	const finalSystemPrompt = systemPrompt ? typeof systemPrompt === "string" ? `${systemPrompt}\n\n${BASE_PROMPT}` : new SystemMessage({ content: [{
		type: "text",
		text: BASE_PROMPT
	}, ...typeof systemPrompt.content === "string" ? [{
		type: "text",
		text: systemPrompt.content
	}] : systemPrompt.content] }) : BASE_PROMPT;
	/**
	* Create backend configuration for filesystem middleware
	* If no backend is provided, use a factory that creates a StateBackend
	*/
	const filesystemBackend = backend ? backend : (config) => new StateBackend(config);
	/**
	* Skills middleware (created conditionally for runtime use)
	*/
	const skillsMiddlewareArray = skills != null && skills.length > 0 ? [createSkillsMiddleware({
		backend: filesystemBackend,
		sources: skills
	})] : [];
	/**
	* Memory middleware (created conditionally for runtime use)
	*/
	const memoryMiddlewareArray = memory != null && memory.length > 0 ? [createMemoryMiddleware({
		backend: filesystemBackend,
		sources: memory
	})] : [];
	/**
	* Process subagents to add SkillsMiddleware for those with their own skills.
	*
	* Custom subagents do NOT inherit skills from the main agent by default.
	* Only the general-purpose subagent inherits the main agent's skills (via defaultMiddleware).
	* If a custom subagent needs skills, it must specify its own `skills` array.
	*/
	const processedSubagents = subagents.map((subagent) => {
		/**
		* CompiledSubAgent - use as-is (already has its own middleware baked in)
		*/
		if (Runnable.isRunnable(subagent)) return subagent;
		/**
		* SubAgent without skills - use as-is
		*/
		if (!("skills" in subagent) || subagent.skills?.length === 0) return subagent;
		/**
		* SubAgent with skills - add SkillsMiddleware BEFORE user's middleware
		* Order: base middleware (via defaultMiddleware)  skills  user's middleware
		* This matches Python's ordering in create_deep_agent
		*/
		const subagentSkillsMiddleware = createSkillsMiddleware({
			backend: filesystemBackend,
			sources: subagent.skills ?? []
		});
		return {
			...subagent,
			middleware: [subagentSkillsMiddleware, ...subagent.middleware || []]
		};
	});
	/**
	* Middleware for custom subagents (does NOT include skills from main agent).
	* Custom subagents must define their own `skills` property to get skills.
	*/
	const subagentMiddleware = [
		todoListMiddleware(),
		createFilesystemMiddleware({ backend: filesystemBackend }),
		summarizationMiddleware({
			model,
			trigger: { tokens: 17e4 },
			keep: { messages: 6 }
		}),
		anthropicPromptCachingMiddleware({ unsupportedModelBehavior: "ignore" }),
		createPatchToolCallsMiddleware()
	];
	/**
	* Return as DeepAgent with proper DeepAgentTypeConfig
	* - Response: InferStructuredResponse<TResponse> (unwraps ToolStrategy<T>/ProviderStrategy<T>  T)
	* - State: undefined (state comes from middleware)
	* - Context: ContextSchema
	* - Middleware: AllMiddleware (built-in + custom + subagent middleware for state inference)
	* - Tools: TTools
	* - Subagents: TSubagents (for type-safe streaming)
	*/
	return createAgent({
		model,
		systemPrompt: finalSystemPrompt,
		tools,
		middleware: [
			...[
				todoListMiddleware(),
				createFilesystemMiddleware({ backend: filesystemBackend }),
				createSubAgentMiddleware({
					defaultModel: model,
					defaultTools: tools,
					defaultMiddleware: subagentMiddleware,
					generalPurposeMiddleware: [...subagentMiddleware, ...skillsMiddlewareArray],
					defaultInterruptOn: interruptOn,
					subagents: processedSubagents,
					generalPurposeAgent: true
				}),
				summarizationMiddleware({
					model,
					trigger: { tokens: 17e4 },
					keep: { messages: 6 }
				}),
				anthropicPromptCachingMiddleware({ unsupportedModelBehavior: "ignore" }),
				createPatchToolCallsMiddleware()
			],
			...skillsMiddlewareArray,
			...memoryMiddlewareArray,
			...interruptOn ? [humanInTheLoopMiddleware({ interruptOn })] : [],
			...customMiddleware
		],
		...responseFormat != null && { responseFormat },
		contextSchema,
		checkpointer,
		store,
		name
	}).withConfig({ recursionLimit: 1e4 });
}

//#endregion
//#region src/config.ts
/**
* Configuration and settings for deepagents.
*
* Provides project detection, path management, and environment configuration
* for skills and agent memory middleware.
*/
/**
* Find the project root by looking for .git directory.
*
* Walks up the directory tree from startPath (or cwd) looking for a .git
* directory, which indicates the project root.
*
* @param startPath - Directory to start searching from. Defaults to current working directory.
* @returns Path to the project root if found, null otherwise.
*/
function findProjectRoot(startPath) {
	let current = path.resolve(startPath || process.cwd());
	while (current !== path.dirname(current)) {
		const gitDir = path.join(current, ".git");
		if (fs$1.existsSync(gitDir)) return current;
		current = path.dirname(current);
	}
	const rootGitDir = path.join(current, ".git");
	if (fs$1.existsSync(rootGitDir)) return current;
	return null;
}
/**
* Validate agent name to prevent invalid filesystem paths and security issues.
*
* @param agentName - The agent name to validate
* @returns True if valid, false otherwise
*/
function isValidAgentName(agentName) {
	if (!agentName || !agentName.trim()) return false;
	return /^[a-zA-Z0-9_\-\s]+$/.test(agentName);
}
/**
* Create a Settings instance with detected environment.
*
* @param options - Configuration options
* @returns Settings instance with project detection and path management
*/
function createSettings(options = {}) {
	const projectRoot = findProjectRoot(options.startPath);
	const userDeepagentsDir = path.join(os.homedir(), ".deepagents");
	return {
		projectRoot,
		userDeepagentsDir,
		hasProject: projectRoot !== null,
		getAgentDir(agentName) {
			if (!isValidAgentName(agentName)) throw new Error(`Invalid agent name: ${JSON.stringify(agentName)}. Agent names can only contain letters, numbers, hyphens, underscores, and spaces.`);
			return path.join(userDeepagentsDir, agentName);
		},
		ensureAgentDir(agentName) {
			const agentDir = this.getAgentDir(agentName);
			fs$1.mkdirSync(agentDir, { recursive: true });
			return agentDir;
		},
		getUserAgentMdPath(agentName) {
			return path.join(this.getAgentDir(agentName), "agent.md");
		},
		getProjectAgentMdPath() {
			if (!projectRoot) return null;
			return path.join(projectRoot, ".deepagents", "agent.md");
		},
		getUserSkillsDir(agentName) {
			return path.join(this.getAgentDir(agentName), "skills");
		},
		ensureUserSkillsDir(agentName) {
			const skillsDir = this.getUserSkillsDir(agentName);
			fs$1.mkdirSync(skillsDir, { recursive: true });
			return skillsDir;
		},
		getProjectSkillsDir() {
			if (!projectRoot) return null;
			return path.join(projectRoot, ".deepagents", "skills");
		},
		ensureProjectSkillsDir() {
			const skillsDir = this.getProjectSkillsDir();
			if (!skillsDir) return null;
			fs$1.mkdirSync(skillsDir, { recursive: true });
			return skillsDir;
		},
		ensureProjectDeepagentsDir() {
			if (!projectRoot) return null;
			const deepagentsDir = path.join(projectRoot, ".deepagents");
			fs$1.mkdirSync(deepagentsDir, { recursive: true });
			return deepagentsDir;
		}
	};
}

//#endregion
//#region src/middleware/agent-memory.ts
/**
* Middleware for loading agent-specific long-term memory into the system prompt.
*
* This middleware loads the agent's long-term memory from agent.md files
* and injects it into the system prompt. Memory is loaded from:
* - User memory: ~/.deepagents/{agent_name}/agent.md
* - Project memory: {project_root}/.deepagents/agent.md
*
* @deprecated Use `createMemoryMiddleware` from `./memory.js` instead.
* This middleware uses direct filesystem access (Node.js fs module) which is not
* portable across backends. The `createMemoryMiddleware` function uses the
* `BackendProtocol` abstraction and follows the AGENTS.md specification.
*
* Migration example:
* ```typescript
* // Before (deprecated):
* import { createAgentMemoryMiddleware } from "./agent-memory.js";
* const middleware = createAgentMemoryMiddleware({ settings, assistantId });
*
* // After (recommended):
* import { createMemoryMiddleware } from "./memory.js";
* import { FilesystemBackend } from "../backends/filesystem.js";
*
* const middleware = createMemoryMiddleware({
*   backend: new FilesystemBackend({ rootDir: "/" }),
*   sources: [
*     `~/.deepagents/${assistantId}/AGENTS.md`,
*     `${projectRoot}/.deepagents/AGENTS.md`,
*   ],
* });
* ```
*/
/**
* State schema for agent memory middleware.
*/
const AgentMemoryStateSchema = z$1.object({
	userMemory: z$1.string().optional(),
	projectMemory: z$1.string().optional()
});
/**
* Default template for memory injection.
*/
const DEFAULT_MEMORY_TEMPLATE = `<user_memory>
{user_memory}
</user_memory>

<project_memory>
{project_memory}
</project_memory>`;
/**
* Long-term Memory Documentation system prompt.
*/
const LONGTERM_MEMORY_SYSTEM_PROMPT = `

## Long-term Memory

Your long-term memory is stored in files on the filesystem and persists across sessions.

**User Memory Location**: \`{agent_dir_absolute}\` (displays as \`{agent_dir_display}\`)
**Project Memory Location**: {project_memory_info}

Your system prompt is loaded from TWO sources at startup:
1. **User agent.md**: \`{agent_dir_absolute}/agent.md\` - Your personal preferences across all projects
2. **Project agent.md**: Loaded from project root if available - Project-specific instructions

Project-specific agent.md is loaded from these locations (both combined if both exist):
- \`[project-root]/.deepagents/agent.md\` (preferred)
- \`[project-root]/agent.md\` (fallback, but also included if both exist)

**When to CHECK/READ memories (CRITICAL - do this FIRST):**
- **At the start of ANY new session**: Check both user and project memories
  - User: \`ls {agent_dir_absolute}\`
  - Project: \`ls {project_deepagents_dir}\` (if in a project)
- **BEFORE answering questions**: If asked "what do you know about X?" or "how do I do Y?", check project memories FIRST, then user
- **When user asks you to do something**: Check if you have project-specific guides or examples
- **When user references past work**: Search project memory files for related context

**Memory-first response pattern:**
1. User asks a question  Check project directory first: \`ls {project_deepagents_dir}\`
2. If relevant files exist  Read them with \`read_file '{project_deepagents_dir}/[filename]'\`
3. Check user memory if needed  \`ls {agent_dir_absolute}\`
4. Base your answer on saved knowledge supplemented by general knowledge

**When to update memories:**
- **IMMEDIATELY when the user describes your role or how you should behave**
- **IMMEDIATELY when the user gives feedback on your work** - Update memories to capture what was wrong and how to do it better
- When the user explicitly asks you to remember something
- When patterns or preferences emerge (coding styles, conventions, workflows)
- After significant work where context would help in future sessions

**Learning from feedback:**
- When user says something is better/worse, capture WHY and encode it as a pattern
- Each correction is a chance to improve permanently - don't just fix the immediate issue, update your instructions
- When user says "you should remember X" or "be careful about Y", treat this as HIGH PRIORITY - update memories IMMEDIATELY
- Look for the underlying principle behind corrections, not just the specific mistake

## Deciding Where to Store Memory

When writing or updating agent memory, decide whether each fact, configuration, or behavior belongs in:

### User Agent File: \`{agent_dir_absolute}/agent.md\`
 Describes the agent's **personality, style, and universal behavior** across all projects.

**Store here:**
- Your general tone and communication style
- Universal coding preferences (formatting, comment style, etc.)
- General workflows and methodologies you follow
- Tool usage patterns that apply everywhere
- Personal preferences that don't change per-project

**Examples:**
- "Be concise and direct in responses"
- "Always use type hints in Python"
- "Prefer functional programming patterns"

### Project Agent File: \`{project_deepagents_dir}/agent.md\`
 Describes **how this specific project works** and **how the agent should behave here only.**

**Store here:**
- Project-specific architecture and design patterns
- Coding conventions specific to this codebase
- Project structure and organization
- Testing strategies for this project
- Deployment processes and workflows
- Team conventions and guidelines

**Examples:**
- "This project uses FastAPI with SQLAlchemy"
- "Tests go in tests/ directory mirroring src/ structure"
- "All API changes require updating OpenAPI spec"

### Project Memory Files: \`{project_deepagents_dir}/*.md\`
 Use for **project-specific reference information** and structured notes.

**Store here:**
- API design documentation
- Architecture decisions and rationale
- Deployment procedures
- Common debugging patterns
- Onboarding information

**Examples:**
- \`{project_deepagents_dir}/api-design.md\` - REST API patterns used
- \`{project_deepagents_dir}/architecture.md\` - System architecture overview
- \`{project_deepagents_dir}/deployment.md\` - How to deploy this project

### File Operations:

**User memory:**
\`\`\`
ls {agent_dir_absolute}                              # List user memory files
read_file '{agent_dir_absolute}/agent.md'            # Read user preferences
edit_file '{agent_dir_absolute}/agent.md' ...        # Update user preferences
\`\`\`

**Project memory (preferred for project-specific information):**
\`\`\`
ls {project_deepagents_dir}                          # List project memory files
read_file '{project_deepagents_dir}/agent.md'        # Read project instructions
edit_file '{project_deepagents_dir}/agent.md' ...    # Update project instructions
write_file '{project_deepagents_dir}/agent.md' ...  # Create project memory file
\`\`\`

**Important**:
- Project memory files are stored in \`.deepagents/\` inside the project root
- Always use absolute paths for file operations
- Check project memories BEFORE user when answering project-specific questions`;
/**
* Create middleware for loading agent-specific long-term memory.
*
* This middleware loads the agent's long-term memory from a file (agent.md)
* and injects it into the system prompt. The memory is loaded once at the
* start of the conversation and stored in state.
*
* @param options - Configuration options
* @returns AgentMiddleware for memory loading and injection
*
* @deprecated Use `createMemoryMiddleware` from `./memory.js` instead.
* This function uses direct filesystem access which limits portability.
*/
function createAgentMemoryMiddleware(options) {
	const { settings, assistantId, systemPromptTemplate } = options;
	const agentDir = settings.getAgentDir(assistantId);
	const agentDirDisplay = `~/.deepagents/${assistantId}`;
	const agentDirAbsolute = agentDir;
	const projectRoot = settings.projectRoot;
	const projectMemoryInfo = projectRoot ? `\`${projectRoot}\` (detected)` : "None (not in a git project)";
	const projectDeepagentsDir = projectRoot ? `${projectRoot}/.deepagents` : "[project-root]/.deepagents (not in a project)";
	const template = systemPromptTemplate || DEFAULT_MEMORY_TEMPLATE;
	return createMiddleware({
		name: "AgentMemoryMiddleware",
		stateSchema: AgentMemoryStateSchema,
		beforeAgent(state) {
			const result = {};
			if (!("userMemory" in state)) {
				const userPath = settings.getUserAgentMdPath(assistantId);
				if (fs$1.existsSync(userPath)) try {
					result.userMemory = fs$1.readFileSync(userPath, "utf-8");
				} catch {}
			}
			if (!("projectMemory" in state)) {
				const projectPath = settings.getProjectAgentMdPath();
				if (projectPath && fs$1.existsSync(projectPath)) try {
					result.projectMemory = fs$1.readFileSync(projectPath, "utf-8");
				} catch {}
			}
			return Object.keys(result).length > 0 ? result : void 0;
		},
		wrapModelCall(request, handler) {
			const userMemory = request.state?.userMemory;
			const projectMemory = request.state?.projectMemory;
			const baseSystemPrompt = request.systemPrompt || "";
			const memorySection = template.replace("{user_memory}", userMemory || "(No user agent.md)").replace("{project_memory}", projectMemory || "(No project agent.md)");
			const memoryDocs = LONGTERM_MEMORY_SYSTEM_PROMPT.replaceAll("{agent_dir_absolute}", agentDirAbsolute).replaceAll("{agent_dir_display}", agentDirDisplay).replaceAll("{project_memory_info}", projectMemoryInfo).replaceAll("{project_deepagents_dir}", projectDeepagentsDir);
			let systemPrompt = memorySection;
			if (baseSystemPrompt) systemPrompt += "\n\n" + baseSystemPrompt;
			systemPrompt += "\n\n" + memoryDocs;
			return handler({
				...request,
				systemPrompt
			});
		}
	});
}

//#endregion
//#region src/skills/loader.ts
/**
* Skill loader for parsing and loading agent skills from SKILL.md files.
*
* This module implements Anthropic's agent skills pattern with YAML frontmatter parsing.
* Each skill is a directory containing a SKILL.md file with:
* - YAML frontmatter (name, description required)
* - Markdown instructions for the agent
* - Optional supporting files (scripts, configs, etc.)
*
* @example
* ```markdown
* ---
* name: web-research
* description: Structured approach to conducting thorough web research
* ---
*
* # Web Research Skill
*
* ## When to Use
* - User asks you to research a topic
* ...
* ```
*
* @see https://agentskills.io/specification
*/
/** Maximum size for SKILL.md files (10MB) */
const MAX_SKILL_FILE_SIZE$1 = 10 * 1024 * 1024;
/** Agent Skills spec constraints */
const MAX_SKILL_NAME_LENGTH$1 = 64;
const MAX_SKILL_DESCRIPTION_LENGTH$1 = 1024;
/** Pattern for validating skill names per Agent Skills spec */
const SKILL_NAME_PATTERN = /^[a-z0-9]+(-[a-z0-9]+)*$/;
/** Pattern for extracting YAML frontmatter */
const FRONTMATTER_PATTERN = /^---\s*\n([\s\S]*?)\n---\s*\n/;
/**
* Check if a path is safely contained within base_dir.
*
* This prevents directory traversal attacks via symlinks or path manipulation.
* The function resolves both paths to their canonical form (following symlinks)
* and verifies that the target path is within the base directory.
*
* @param targetPath - The path to validate
* @param baseDir - The base directory that should contain the path
* @returns True if the path is safely within baseDir, false otherwise
*/
function isSafePath(targetPath, baseDir) {
	try {
		const resolvedPath = fs$1.realpathSync(targetPath);
		const resolvedBase = fs$1.realpathSync(baseDir);
		return resolvedPath.startsWith(resolvedBase + path.sep) || resolvedPath === resolvedBase;
	} catch {
		return false;
	}
}
/**
* Validate skill name per Agent Skills spec.
*
* Requirements:
* - Max 64 characters
* - Lowercase alphanumeric and hyphens only (a-z, 0-9, -)
* - Cannot start or end with hyphen
* - No consecutive hyphens
* - Must match parent directory name
*
* @param name - The skill name from YAML frontmatter
* @param directoryName - The parent directory name
* @returns Validation result with error message if invalid
*/
function validateSkillName(name, directoryName) {
	if (!name) return {
		valid: false,
		error: "name is required"
	};
	if (name.length > MAX_SKILL_NAME_LENGTH$1) return {
		valid: false,
		error: "name exceeds 64 characters"
	};
	if (!SKILL_NAME_PATTERN.test(name)) return {
		valid: false,
		error: "name must be lowercase alphanumeric with single hyphens only"
	};
	if (name !== directoryName) return {
		valid: false,
		error: `name '${name}' must match directory name '${directoryName}'`
	};
	return { valid: true };
}
/**
* Parse YAML frontmatter from content.
*
* @param content - The file content
* @returns Parsed frontmatter object, or null if parsing fails
*/
function parseFrontmatter(content) {
	const match = content.match(FRONTMATTER_PATTERN);
	if (!match) return null;
	try {
		const parsed = yaml.parse(match[1]);
		return typeof parsed === "object" && parsed !== null ? parsed : null;
	} catch {
		return null;
	}
}
/**
* Parse YAML frontmatter from a SKILL.md file per Agent Skills spec.
*
* @param skillMdPath - Path to the SKILL.md file
* @param source - Source of the skill ('user' or 'project')
* @returns SkillMetadata with all fields, or null if parsing fails
*/
function parseSkillMetadata(skillMdPath, source) {
	try {
		const stats = fs$1.statSync(skillMdPath);
		if (stats.size > MAX_SKILL_FILE_SIZE$1) {
			console.warn(`Skipping ${skillMdPath}: file too large (${stats.size} bytes)`);
			return null;
		}
		const frontmatter = parseFrontmatter(fs$1.readFileSync(skillMdPath, "utf-8"));
		if (!frontmatter) {
			console.warn(`Skipping ${skillMdPath}: no valid YAML frontmatter found`);
			return null;
		}
		const name = frontmatter.name;
		const description = frontmatter.description;
		if (!name || !description) {
			console.warn(`Skipping ${skillMdPath}: missing required 'name' or 'description'`);
			return null;
		}
		const directoryName = path.basename(path.dirname(skillMdPath));
		const validation = validateSkillName(String(name), directoryName);
		if (!validation.valid) console.warn(`Skill '${name}' in ${skillMdPath} does not follow Agent Skills spec: ${validation.error}. Consider renaming to be spec-compliant.`);
		let descriptionStr = String(description);
		if (descriptionStr.length > MAX_SKILL_DESCRIPTION_LENGTH$1) {
			console.warn(`Description exceeds ${MAX_SKILL_DESCRIPTION_LENGTH$1} chars in ${skillMdPath}, truncating`);
			descriptionStr = descriptionStr.slice(0, MAX_SKILL_DESCRIPTION_LENGTH$1);
		}
		return {
			name: String(name),
			description: descriptionStr,
			path: skillMdPath,
			source,
			license: frontmatter.license ? String(frontmatter.license) : void 0,
			compatibility: frontmatter.compatibility ? String(frontmatter.compatibility) : void 0,
			metadata: frontmatter.metadata && typeof frontmatter.metadata === "object" ? frontmatter.metadata : void 0,
			allowedTools: frontmatter["allowed-tools"] ? String(frontmatter["allowed-tools"]) : void 0
		};
	} catch (error) {
		console.warn(`Error reading ${skillMdPath}: ${error}`);
		return null;
	}
}
/**
* List all skills from a single skills directory (internal helper).
*
* Scans the skills directory for subdirectories containing SKILL.md files,
* parses YAML frontmatter, and returns skill metadata.
*
* Skills are organized as:
* ```
* skills/
*  skill-name/
*     SKILL.md        # Required: instructions with YAML frontmatter
*     script.py       # Optional: supporting files
*     config.json     # Optional: supporting files
* ```
*
* @param skillsDir - Path to the skills directory
* @param source - Source of the skills ('user' or 'project')
* @returns List of skill metadata
*/
function listSkillsFromDir(skillsDir, source) {
	const expandedDir = skillsDir.startsWith("~") ? path.join(process.env.HOME || process.env.USERPROFILE || "", skillsDir.slice(1)) : skillsDir;
	if (!fs$1.existsSync(expandedDir)) return [];
	let resolvedBase;
	try {
		resolvedBase = fs$1.realpathSync(expandedDir);
	} catch {
		return [];
	}
	const skills = [];
	let entries;
	try {
		entries = fs$1.readdirSync(resolvedBase, { withFileTypes: true });
	} catch {
		return [];
	}
	for (const entry of entries) {
		const skillDir = path.join(resolvedBase, entry.name);
		if (!isSafePath(skillDir, resolvedBase)) continue;
		if (!entry.isDirectory()) continue;
		const skillMdPath = path.join(skillDir, "SKILL.md");
		if (!fs$1.existsSync(skillMdPath)) continue;
		if (!isSafePath(skillMdPath, resolvedBase)) continue;
		const metadata = parseSkillMetadata(skillMdPath, source);
		if (metadata) skills.push(metadata);
	}
	return skills;
}
/**
* List skills from user and/or project directories.
*
* When both directories are provided, project skills with the same name as
* user skills will override them.
*
* @param options - Options specifying which directories to search
* @returns Merged list of skill metadata from both sources, with project skills
*          taking precedence over user skills when names conflict
*/
function listSkills(options) {
	const allSkills = /* @__PURE__ */ new Map();
	if (options.userSkillsDir) {
		const userSkills = listSkillsFromDir(options.userSkillsDir, "user");
		for (const skill of userSkills) allSkills.set(skill.name, skill);
	}
	if (options.projectSkillsDir) {
		const projectSkills = listSkillsFromDir(options.projectSkillsDir, "project");
		for (const skill of projectSkills) allSkills.set(skill.name, skill);
	}
	return Array.from(allSkills.values());
}

//#endregion
export { BaseSandbox, CompositeBackend, DEFAULT_GENERAL_PURPOSE_DESCRIPTION, DEFAULT_SUBAGENT_PROMPT, FilesystemBackend, GENERAL_PURPOSE_SUBAGENT, MAX_SKILL_DESCRIPTION_LENGTH, MAX_SKILL_FILE_SIZE, MAX_SKILL_NAME_LENGTH, SandboxError, StateBackend, StoreBackend, TASK_SYSTEM_PROMPT, createAgentMemoryMiddleware, createDeepAgent, createFilesystemMiddleware, createMemoryMiddleware, createPatchToolCallsMiddleware, createSettings, createSkillsMiddleware, createSubAgentMiddleware, filesValue, findProjectRoot, isSandboxBackend, listSkills, parseSkillMetadata };
//# sourceMappingURL=index.js.map